{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "genetic-diamond",
   "metadata": {},
   "source": [
    "# DataFrame Practice\n",
    "Here we will use spark dataframe on actual datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "worse-sunrise",
   "metadata": {},
   "source": [
    "## 1. DataFrames with JSON files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "collaborative-makeup",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.master('local[*]').appName('spark_df_data').getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "simplified-frederick",
   "metadata": {},
   "source": [
    "We can use the `json` reader to read in many json files at once.  Each json file becomes a single row in the resulting DataFrame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sustainable-lemon",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.json('structured-2018-01-14-neworleans/*.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "matched-midnight",
   "metadata": {},
   "outputs": [],
   "source": [
    "# how many rows do we have (i.e. how many json files did we read?)\n",
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "executed-southwest",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the .columns member to list the columns out\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "joint-daily",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can select columns just like in SQL\n",
    "df.select(['map', 'mode', 'title']).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "polyphonic-idaho",
   "metadata": {},
   "outputs": [],
   "source": [
    "# selecting a single column is similar to pandas\n",
    "df.mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dietary-renaissance",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['mode']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "built-surgeon",
   "metadata": {},
   "source": [
    "We can perform standard aggregations (e.g. avg, min, max, etc).\n",
    "\n",
    "However, we always need to perform a `groupBy()`, even if we aren't grouping:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "seasonal-egypt",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupBy().avg('duration_ms').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "informative-collect",
   "metadata": {},
   "source": [
    "There are actually quite a few alternative syntaxes to do the same thing.  Sometimes this gets a little confusing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "spread-conversation",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupBy().agg({'duration_ms': 'avg'}).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe17690c-0802-4a31-9067-468c4c93225c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as fn\n",
    "\n",
    "#select avg(column) as avg_column\n",
    "#avg(column) avg_column\n",
    "\n",
    "df.groupBy().agg(fn.avg('duration_ms')).show()\n",
    "df.groupBy().agg(fn.avg(fn.col('duration_ms'))).show()\n",
    "df.groupBy().agg(fn.avg(fn.col('duration_ms').alias('duration'))).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "otherwise-documentary",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupBy().agg(fn.avg(df.duration_ms)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "heard-walker",
   "metadata": {},
   "source": [
    "There are many useful functions in the `pyspark.sql.functions` module.  We will use some of them, like we did above.\n",
    "\n",
    "Obviously, we can also perform aggregations over actual groups.  Here's an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "skilled-hometown",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupBy('mode').avg('duration_ms').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "amended-gross",
   "metadata": {},
   "source": [
    "Json data is usually nested, which is a little \"weird\" when you are trying to analyze it using SQL-like tables.\n",
    "\n",
    "For example, in the CWL json the `teams` field is actually a list of length 2 (one for each team):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "virgin-friendly",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select('teams').limit(5).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "spiritual-advancement",
   "metadata": {},
   "source": [
    "Sometimes it can be better to do a `take` than a `show` so that we can see the nested structure better:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "synthetic-silver",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select('teams').take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acting-difference",
   "metadata": {},
   "source": [
    "The .explode() function is a very useful way to \"denormalize\" the data.  TL;DR explodes a nested list into multiple rows (at the cost of introducing some redundancy):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "future-acceptance",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as fn\n",
    "teams_df = df.select('id', fn.explode('teams'))\n",
    "teams_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "prompt-adolescent",
   "metadata": {},
   "source": [
    "Actually, it is better to rename our column to \"team\" because we exploded a list of 2 teams into a 2 separate rows each containing a team.  We use `alias` to rename:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imposed-theory",
   "metadata": {},
   "outputs": [],
   "source": [
    "teams_df = df.select('id', fn.explode('teams').alias('team'))\n",
    "teams_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unauthorized-sending",
   "metadata": {},
   "outputs": [],
   "source": [
    "teams_df.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "concrete-maria",
   "metadata": {},
   "source": [
    "Notice that the \"team\" column is still nested.  This isn't really limiting, though.  We can use the col.field syntax to get at the subfields:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stone-invitation",
   "metadata": {},
   "outputs": [],
   "source": [
    "teams_df.select('id', 'team.name').show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "authentic-reader",
   "metadata": {},
   "source": [
    "If we want to rename the column then we have to use a noisier syntax:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stuffed-terrorism",
   "metadata": {},
   "outputs": [],
   "source": [
    "teams_df.select('id', teams_df.team['name'].alias('team_name')).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "suffering-colonial",
   "metadata": {},
   "source": [
    "... or we could've used the `.withColumnRenamed()` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "upset-reach",
   "metadata": {},
   "outputs": [],
   "source": [
    "teams_df.select('id', 'team.name').withColumnRenamed('name', 'team_name').show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "insured-wallet",
   "metadata": {},
   "source": [
    "Let's explode the `players` nested field:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "above-noise",
   "metadata": {},
   "outputs": [],
   "source": [
    "players_df = df.select('id', fn.explode('players'))\n",
    "players_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "important-senior",
   "metadata": {},
   "source": [
    "To reduce the number of joins we'll have to make, let's redo this last step but keep some more fields (at the cost of redundancy).  This is \"denormalization\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "generic-office",
   "metadata": {},
   "outputs": [],
   "source": [
    "players_df = df.select('id',\n",
    "                       'platform',\n",
    "                       'title',\n",
    "                       'mode',\n",
    "                       'map',\n",
    "                       'start_time_s',\n",
    "                       'end_time_s',                                                                                                                                                                                                                                                                                                                                                                                              \n",
    "                       'duration_ms',\n",
    "                       fn.explode('players').alias('player'))\n",
    "players_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "therapeutic-suspension",
   "metadata": {},
   "outputs": [],
   "source": [
    "players_df.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "collect-quantity",
   "metadata": {},
   "source": [
    "You can join just like in SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "indie-scene",
   "metadata": {},
   "outputs": [],
   "source": [
    "joined_df = players_df.join(teams_df, \n",
    "                            [players_df.id == teams_df.id, \n",
    "                             players_df.player['team'] == teams_df.team['name']])\n",
    "joined_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd452e41-459f-4354-aefa-1fbf1da1ee56",
   "metadata": {},
   "outputs": [],
   "source": [
    "#joined_df = players_df.join(teams_df, \n",
    "#                            ['id'])\n",
    "#joined_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "collect-collaboration",
   "metadata": {},
   "source": [
    "Filtering is also easy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "liberal-alarm",
   "metadata": {},
   "outputs": [],
   "source": [
    "joined_df.select('mode').distinct().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fourth-advertising",
   "metadata": {},
   "outputs": [],
   "source": [
    "ctf_df = joined_df.filter(joined_df.mode == 'Search & Destroy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "optional-drive",
   "metadata": {},
   "outputs": [],
   "source": [
    "ctf_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "indonesian-delivery",
   "metadata": {},
   "outputs": [],
   "source": [
    "joined_df.groupBy('mode').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adaptive-tuesday",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupBy('mode').count().show()\n",
    "#each game has eight players in total"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "brilliant-railway",
   "metadata": {},
   "source": [
    "## 2. Prepare and understand data for modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "explicit-championship",
   "metadata": {},
   "source": [
    "### Duplicates\n",
    "\n",
    "Consider the following example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "destroyed-australian",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "df_olympia = spark.read.options(header=True, inferSchema=True).csv('./olympia2016_athletes_ex.csv')\n",
    "df_olympia.show(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "judicial-purchase",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_olympia.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "married-bulgaria",
   "metadata": {},
   "source": [
    "Check for duplicates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "owned-northeast",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "print(df_olympia.count())\n",
    "print(df_olympia.distinct().count())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "israeli-professor",
   "metadata": {},
   "source": [
    "If these two numbers differ - you have rows that are exact copies of each other. We can drop these rows by using the `.dropDuplicates(...)` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "handy-classroom",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "df_olympia = df_olympia.dropDuplicates()\n",
    "df_olympia.show(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "integrated-johnston",
   "metadata": {},
   "source": [
    "Let's confirm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "searching-mention",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "print(df_olympia.count())\n",
    "print(df_olympia.distinct().count())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "decent-sphere",
   "metadata": {},
   "source": [
    "We still have one more duplicate. We will use the `.dropDuplicates(...)` but add the `subset` parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "higher-lawrence",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "#drop duplicates based on a subset of columns\n",
    "#df_olympia = df_olympia.dropDuplicates(subset=df.columns[1:])\n",
    "#df_olympia.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "agreed-dakota",
   "metadata": {},
   "source": [
    "To calculate the total and distinct number of IDs in one step we can use the `.agg(...)` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "oriented-marsh",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as fn\n",
    "\n",
    "df_olympia.agg(\n",
    "    fn.count('id').alias('count'),\n",
    "    fn.countDistinct('id').alias('distinct')\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alpine-presentation",
   "metadata": {},
   "source": [
    "Give each row a unique ID. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "strong-exhibit",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "df_olympia.withColumn('new_id', fn.monotonically_increasing_id()).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rough-loading",
   "metadata": {},
   "source": [
    "### Missing observations\n",
    "\n",
    "Consider a similar example to the one we presented above."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "collect-establishment",
   "metadata": {},
   "source": [
    "To find the number of missing observations per row we can use the following snippet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acute-causing",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "df_olympia.rdd.repartition(4).map(\n",
    "    lambda row: (row['id'], sum([c == None for c in row]))\n",
    ").take(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "southern-volume",
   "metadata": {},
   "source": [
    "Let's see what values are missing so when we count missing observations in columns we can decide whether to drop the observation altogether or impute some of the observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "flush-winter",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "df_olympia.where('id == 222063859').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "elder-nepal",
   "metadata": {},
   "source": [
    "What is the percentage of missing observations we see in each column?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cheap-discipline",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "df_olympia.agg(*[\n",
    "    (1 - (fn.count(c) / fn.count('*'))).alias(c + '_missing')\n",
    "    for c in df_olympia.columns\n",
    "]).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "retired-robinson",
   "metadata": {},
   "source": [
    "We will drop the `'income'` feature as most of its values are missing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unlimited-factory",
   "metadata": {},
   "source": [
    "To drop the observations instead you can use the `.dropna(...)` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "national-vertex",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "df_olympia.dropna().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "applied-favor",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_olympia.dropna().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "clinical-installation",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_olympia.fillna(0, ['gold', 'silver', 'bronze']).where('id = 256673338').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "terminal-underwear",
   "metadata": {},
   "source": [
    "To impute a mean, median or other *calculated* value you need to first calculate the value, create a dict with such values, and then pass it to the `.fillna(...)` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aggressive-lawrence",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "df_olympia = df_olympia.fillna(0, ['gold', 'silver', 'bronze'])\n",
    "df_olympia.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tired-monday",
   "metadata": {},
   "outputs": [],
   "source": [
    "means = df_olympia.agg(fn.mean('height').alias('height')).toPandas().to_dict('records')[0]\n",
    "df_olympia.fillna(means, 'height').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "russian-farmer",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_olympia.agg(fn.mean('height').alias('height')).toPandas().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "serial-dayton",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Window\n",
    "\n",
    "window = Window().partitionBy('sport', 'sex')\n",
    "df_olympia.withColumn('height', fn.when(fn.col('height').isNull(), fn.avg(fn.col('height')).over(window)).otherwise(fn.col('height'))).where('id = 222063859').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "substantial-animation",
   "metadata": {},
   "outputs": [],
   "source": [
    "window = Window().partitionBy('sport', 'sex')\n",
    "df_olympia = df_olympia.withColumn('height', fn.when(fn.col('height').isNull(), fn.avg(fn.col('height')).over(window)).otherwise(fn.col('height')))\n",
    "df_olympia.where('id = 222063859').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "special-irish",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_olympia = df_olympia.withColumn('weight', fn.when(fn.col('weight').isNull(), fn.avg(fn.col('weight')).over(window)).otherwise(fn.col('weight')))\n",
    "df_olympia.where('id = 587168078').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "generic-geneva",
   "metadata": {},
   "source": [
    "### Outliers\n",
    "\n",
    "Consider another simple example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "blocked-tanzania",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "df_olympia.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "behavioral-blind",
   "metadata": {},
   "source": [
    "First, we calculate the lower and upper *cut off* points for each feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "important-enforcement",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "cols = ['height', 'weight']\n",
    "bounds = {}\n",
    "\n",
    "for col in cols:\n",
    "    quantiles = df_olympia.approxQuantile(col, [0.25, 0.75], 0.05)\n",
    "    IQR = quantiles[1] - quantiles[0]\n",
    "    bounds[col] = [quantiles[0] - 1.5 * IQR, quantiles[1] + 1.5 * IQR]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "moral-administration",
   "metadata": {},
   "source": [
    "The `bounds` dictionary holds the lower and upper bounds for each feature. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "phantom-tumor",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "bounds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "joint-baking",
   "metadata": {},
   "source": [
    "Let's now use it to flag our outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pressing-emerald",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "df_outliers = df_olympia.select(['id'] + [\n",
    "    (\n",
    "        (df_olympia[c] < bounds[c][0]) | \n",
    "        (df_olympia[c] > bounds[c][1])\n",
    "    ).alias(c + '_o') for c in cols\n",
    "])\n",
    "df_outliers.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "first-rating",
   "metadata": {},
   "source": [
    "We have two outliers in the `weight` feature and two in the `age` feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "immune-india",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "df_olympia_outliers = df_olympia.join(df_outliers, 'id')\n",
    "df_olympia_outliers.filter('height_o').show()\n",
    "df_olympia_outliers.filter('weight_o').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "modern-explorer",
   "metadata": {},
   "outputs": [],
   "source": [
    "#save the cleaned data\n",
    "df_olympia.write.mode('overwrite').option('header', 'True').csv('./olympia2016_athletes_cleaned.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "forbidden-crazy",
   "metadata": {},
   "source": [
    "## 3. Understand your data\n",
    "\n",
    "### Descriptive statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "analyzed-reader",
   "metadata": {},
   "source": [
    "Load our data and convert it to a Spark DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "social-cement",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_olympia = spark.read.options(header=True, inferSchema=True).csv('./olympia2016_athletes_cleaned.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nasty-coast",
   "metadata": {},
   "source": [
    "Next, we read the data in."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "weekly-connection",
   "metadata": {},
   "source": [
    "Following, we create the schema for our `DataFrame`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fluid-drove",
   "metadata": {},
   "source": [
    "Finally, we create our `DataFrame`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "timely-laundry",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "df_olympia.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "driven-community",
   "metadata": {},
   "source": [
    "Now that the dataframe is ready we can calculate the basic descriptive statistics for our dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "boxed-arabic",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "df_olympia.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "compressed-decline",
   "metadata": {},
   "source": [
    "For categorical columns we will count the frequencies of their values using `.groupby(...)` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "elect-graduate",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "df_olympia.groupBy('sex').count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "champion-scheme",
   "metadata": {},
   "source": [
    "For the truly numerical features we can use the `.describe()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "subject-simpson",
   "metadata": {},
   "outputs": [],
   "source": [
    "medals = ['gold', 'silver', 'bronze']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "israeli-boost",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "df_olympia.describe(medals).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "medium-cosmetic",
   "metadata": {},
   "source": [
    "Here's how you check skewness (we will do it for the `'balance'` feature only)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cloudy-sierra",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "df_olympia.agg({'gold': 'skewness'}).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beneficial-volume",
   "metadata": {},
   "source": [
    "Which player won the most gold medals?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "scientific-parade",
   "metadata": {},
   "outputs": [],
   "source": [
    "win = Window.partitionBy()\n",
    "df_olympia.withColumn(\"max_gold\", fn.max(\"gold\").over(win)).filter(\"max_gold = gold\").drop(\"max_gold\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hybrid-potential",
   "metadata": {},
   "outputs": [],
   "source": [
    "win = Window.partitionBy('nationality')\n",
    "df_olympia.withColumn(\"max_gold\", fn.max(\"gold\").over(win)).filter(\"max_gold = gold\").drop(\"max_gold\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "furnished-salem",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_olympia.groupBy('nationality').count().show()\n",
    "df_olympia.groupBy('nationality').agg(fn.count('nationality').alias('count_player')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "furnished-address",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_olympia.groupBy('nationality').avg('height').show()\n",
    "df_olympia.groupBy('nationality').agg(fn.avg('height').alias('avg_height')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "featured-harris",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_olympia.groupBy('nationality').sum('gold').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "right-questionnaire",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_olympia.createOrReplaceTempView('df_olympia')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prescription-palace",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql('select * from df_olympia where gold = (select max(gold) from df_olympia)').show()\n",
    "spark.sql('select * from df_olympia a, (select nationality, max(gold) as max_gold from df_olympia group by nationality) b where a.nationality=b.nationality and a.gold = b.max_gold').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "precise-trout",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql('select nationality, count(*) from df_olympia group by nationality').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "revised-causing",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql('select nationality, avg(height) from df_olympia group by nationality').show()\n",
    "spark.sql('select nationality, avg(height) as avg_height from df_olympia group by nationality').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "violent-playlist",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql('select nationality, sum(gold) from df_olympia group by nationality').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "strange-breakdown",
   "metadata": {},
   "source": [
    "### Correlations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "medium-problem",
   "metadata": {},
   "source": [
    "Calculating correlations in PySpark is very easy once your data is in a DataFrame form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wicked-start",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "df_olympia.corr('height', 'gold')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "considerable-apparel",
   "metadata": {},
   "source": [
    "In order to create a correlations matrix you can use the script below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "entertaining-savage",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "hw_medals = ['height', 'weight', 'gold', 'silver', 'bronze']\n",
    "len_hw_medals = len(hw_medals)\n",
    "\n",
    "corr = []\n",
    "\n",
    "for i in range(0, len_hw_medals):\n",
    "    \n",
    "    temp = [None] * i\n",
    "    \n",
    "    for j in range(i, len_hw_medals):\n",
    "        temp.append(df_olympia.corr(hw_medals[i], hw_medals[j]))\n",
    "    corr.append(temp)\n",
    "    \n",
    "corr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "spread-harbor",
   "metadata": {},
   "source": [
    "### Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rubber-nicaragua",
   "metadata": {},
   "source": [
    "First, let's load the modules and set them up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "polish-tonight",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "standing-norwegian",
   "metadata": {},
   "source": [
    "### Histograms\n",
    "\n",
    "Aggreagate the data in workers and return aggregated list of cut-off points and counts in each bin of the histogram to the driver."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pending-purchase",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "hists = df_olympia.select('gold').rdd.flatMap(lambda row: row).histogram(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pregnant-professor",
   "metadata": {},
   "outputs": [],
   "source": [
    "hists"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mineral-organizer",
   "metadata": {},
   "source": [
    "To plot the histogram you can simply call the matplotlib like below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "executed-pollution",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "data = {\n",
    "    'bins': hists[0][:-1],\n",
    "    'freq': hists[1]\n",
    "}\n",
    "\n",
    "fig = plt.figure(figsize=(12,9))\n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "ax.bar(data['bins'], data['freq'])\n",
    "ax.set_title('Histogram of gold medals')\n",
    "\n",
    "#plt.savefig('gold_hist.png', dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "secure-binding",
   "metadata": {},
   "source": [
    "In a similar manner, a histogram can be create with Bokeh."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ahead-estonia",
   "metadata": {},
   "source": [
    "If your data is small enough to fit on the driver (although we would argue it would normally be faster to use the method showed above) you can bring the data and use the `.hist(...)` (from Matplotlib) or `.Histogram(...)` (from Bokeh) methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "blond-mainstream",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_to_driver = df_olympia.select('gold').rdd.flatMap(lambda row: row).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "canadian-textbook",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(12,9))\n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "\n",
    "ax.hist(data_to_driver, bins=20)\n",
    "ax.set_title('Histogram of gold medals using .hist()')\n",
    "\n",
    "#plt.savefig('Hist_Gold_2.png', dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "breathing-nickel",
   "metadata": {},
   "source": [
    "### Interactions between features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "foster-target",
   "metadata": {},
   "source": [
    "In this example we will sample our fraud dataset at 1% given gender as strata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "written-apparel",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_col = ['height', 'gold']\n",
    "data_sample = df_olympia.sampleBy('sex', {'female': 0.1, 'male': 0.1}).select(plot_col)\n",
    "data_multi = dict([\n",
    "    (elem, data_sample.select(elem).rdd.flatMap(lambda row: row).collect()) \n",
    "    for elem in plot_col\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "governmental-certification",
   "metadata": {},
   "source": [
    "To put multiple 2D charts in one go you can use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "interpreted-shelf",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(12,9))\n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "ax.scatter(data_multi['height'], data_multi['gold'])\n",
    "ax.set_title('Scatter plot of height and gold medals')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cd71165-e348-4293-a4ca-2a592f26f77c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
