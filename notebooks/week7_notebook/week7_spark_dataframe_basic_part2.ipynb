{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "consolidated-compression",
   "metadata": {},
   "source": [
    "# Spark DataFrames and SQL Basic Part 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "convinced-audio",
   "metadata": {},
   "source": [
    "## 1. More DataFrame Operations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "existing-laugh",
   "metadata": {},
   "source": [
    "Let's manually load some data that describes purchase events.  As we've seen, there are many ways to load data manually.  Let's create an RDD first and then convert to a DataFrame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "proper-cover",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.master('local[*]').appName('week 7 spark').getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext\n",
    "purchases_rdd = sc.parallelize([\n",
    "(\"Geoffrey\", \"2016-04-22\", \"A\", \"apples\", 1, 50.00, \"android\"),\n",
    "(\"Geoffrey\", \"2016-05-03\", \"B\", \"Lamp\", 2, 38.00, \"android\"),\n",
    "(\"Geoffrey\", \"2016-05-03\", \"D\", \"Solar Pannel\", 1, 29.00, \"windows\"),\n",
    "(\"Geoffrey\", \"2016-05-03\", \"A\", \"apples\", 3, 50.00, \"android\"),\n",
    "(\"Geoffrey\", \"2016-05-03\", \"C\", \"Rice\", 5, 15.00, \"android\"),\n",
    "(\"Geoffrey\", \"2016-06-05\", \"A\", \"apples\", 5, 50.00, \"windows\"),\n",
    "(\"Geoffrey\", \"2016-06-05\", \"A\", \"bananas\", 5, 55.00, \"windows\"),\n",
    "(\"Geoffrey\", \"2016-06-15\", \"Y\", \"Motor skate\", 7, 68.00, \"windows\"),\n",
    "(\"Geoffrey\", \"2016-06-15\", \"E\", \"Book: The noose\", 1, 125.00, \"android\"),\n",
    "(\"Yann\", \"2016-04-22\", \"B\", \"Lamp\", 1, 38.00, \"ios\"),\n",
    "(\"Yann\", \"2016-05-03\", \"Y\", \"Motor skate\", 1, 68.00, \"ios\"),\n",
    "(\"Yann\", \"2016-05-03\", \"D\", \"Recycle bin\", 5, 27.00, \"macos\"),\n",
    "(\"Yann\", \"2016-05-03\", \"C\", \"Rice\", 15, 15.00, \"macos\"),\n",
    "(\"Yann\", \"2016-04-02\", \"A\", \"bananas\", 3, 55.00, \"macos\"),\n",
    "(\"Yann\", \"2016-04-02\", \"B\", \"Lamp\", 2, 38.00, \"macos\"),\n",
    "(\"Yann\", \"2016-04-03\", \"E\", \"Book: Crime and Punishment\", 5, 100.00, \"macos\"),\n",
    "(\"Yann\", \"2016-04-13\", \"E\", \"Book: The noose\", 5, 125.00, \"macos\"),\n",
    "(\"Yann\", \"2016-04-27\", \"D\", \"Solar Pannel\", 5, 29.00, \"ios\"),\n",
    "(\"Yann\", \"2016-05-27\", \"D\", \"Recycle bin\", 5, 27.00, \"ios\"),\n",
    "(\"Yann\", \"2016-05-27\", \"A\", \"bananas\", 3, 55.00, \"ios\"),\n",
    "(\"Yann\", \"2016-05-01\", \"Y\", \"Motor skate\", 1, 68.00, \"ios\"),\n",
    "(\"Yann\", \"2016-06-07\", \"Z\", \"space ship\", 1, 227.00, \"ios\"),\n",
    "(\"Yoshua\", \"2016-02-07\", \"Z\", \"space ship\", 2, 227.00, \"windows\"),\n",
    "(\"Yoshua\", \"2016-02-14\", \"A\", \"bananas\", 9, 55.00, \"windows\"),\n",
    "(\"Yoshua\", \"2016-02-14\", \"B\", \"Lamp\", 2, 38.00, \"windows\"),\n",
    "(\"Yoshua\", \"2016-02-14\", \"A\", \"apples\", 10, 55.00, \"ios\"),\n",
    "(\"Yoshua\", \"2016-03-07\", \"Z\", \"space ship\", 5, 227.00, \"ios\"),\n",
    "(\"Yoshua\", \"2016-04-07\", \"Y\", \"Motor skate\", 4, 68.00, \"windows\"),\n",
    "(\"Yoshua\", \"2016-04-07\", \"D\", \"Recycle bin\", 5, 27.00, \"ios\"),\n",
    "(\"Yoshua\", \"2016-04-07\", \"C\", \"Rice\", 5, 15.00, \"ios\"),\n",
    "(\"Yoshua\", \"2016-04-07\", \"A\", \"bananas\", 9, 55.00, \"windows\"),\n",
    "(\"Yoshua\", \"2016-04-07\", \"D\", \"Solar Pannel\", 1, 29.00, \"windows\"),\n",
    "(\"Jurgen\", \"2016-05-01\", \"Z\", \"space ship\", 1, 227.00, \"macos\"),\n",
    "(\"Jurgen\", \"2016-05-01\", \"A\", \"bananas\", 5, 55.00, \"macos\"),\n",
    "(\"Jurgen\", \"2016-05-08\", \"A\", \"bananas\", 5, 55.00, \"macos\"),\n",
    "(\"Jurgen\", \"2016-05-08\", \"Y\", \"Motor skate\", 1, 68.00, \"android\"),\n",
    "(\"Jurgen\", \"2016-06-05\", \"A\", \"bananas\", 5, 55.00, \"android\"),\n",
    "(\"Jurgen\", \"2016-06-05\", \"C\", \"Rice\", 5, 15.00, \"windows\"),\n",
    "(\"Jurgen\", \"2016-06-05\", \"Y\", \"Motor skate\", 2, 68.00, \"windows\"),\n",
    "(\"Jurgen\", \"2016-06-05\", \"D\", \"Recycle bin\", 5, 27.00, \"windows\"),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "automatic-seeking",
   "metadata": {},
   "outputs": [],
   "source": [
    "column_names = [\"customer_name\", \"date\", \"category\", \"product_name\", \"quantity\", \"price\", \"channel\"]\n",
    "purchases_df = purchases_rdd.toDF(column_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "diverse-column",
   "metadata": {},
   "source": [
    "We could use `.show(5)`, `.take(5)` or `.head(5)`, but Pandas actually has prettier output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "decimal-proportion",
   "metadata": {},
   "outputs": [],
   "source": [
    "purchases_df.limit(5).toPandas().head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "governing-slope",
   "metadata": {},
   "source": [
    "Let's check the distinct products that are being purchased by our customers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "iraqi-rachel",
   "metadata": {},
   "outputs": [],
   "source": [
    "purchases_df.select('product_name').distinct().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sorted-graduation",
   "metadata": {},
   "source": [
    "## Summary statistics on certain columns\n",
    "\n",
    "A valuable way to get a quick look at some data is to use the `.describe()` method.  This will give some very basic statistics about the columns that I specify:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "considered-palestinian",
   "metadata": {},
   "outputs": [],
   "source": [
    "purchases_df.describe('quantity', 'price').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "physical-shore",
   "metadata": {},
   "source": [
    "## Contingency tables\n",
    "\n",
    "Recall in statistics we have the concept of a \"contingency table\".  In DataFrames we use the `.crosstab()` method to produce one.  This can be a useful way to look at data, but we need to be careful with interpretation here:  this only counts *rows*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "quarterly-inspector",
   "metadata": {},
   "outputs": [],
   "source": [
    "product_freq = purchases_df.crosstab('customer_name', 'product_name')\n",
    "product_freq.toPandas().head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "selective-bridal",
   "metadata": {},
   "source": [
    "Let's look at the columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hindu-victor",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = product_freq.columns\n",
    "cols"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "biblical-australian",
   "metadata": {},
   "source": [
    "So now we can just pass these column names to `.describe()` to get some basic purchase frequency stats:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "valid-handy",
   "metadata": {},
   "outputs": [],
   "source": [
    "product_freq.describe(cols[1:]).toPandas().head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "defined-neighborhood",
   "metadata": {},
   "source": [
    "`.describe()` should ONLY be used for exploratory analysis.  If we really wanted to get the average number of purchase events per product (to be used in further calculations) then we should perform an explicit aggregation ourselves:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exposed-palace",
   "metadata": {},
   "outputs": [],
   "source": [
    "product_count = purchases_df.groupBy('customer_name', 'product_name').count()\n",
    "product_count.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ignored-turkey",
   "metadata": {},
   "outputs": [],
   "source": [
    "product_count = product_count.withColumnRenamed('count', 'num_purchase_events')\n",
    "product_count.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pressing-brush",
   "metadata": {},
   "source": [
    "Let's compute the average number of purchase events per product:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "operational-texas",
   "metadata": {},
   "outputs": [],
   "source": [
    "product_count.groupBy('product_name').avg('num_purchase_events').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wireless-covering",
   "metadata": {},
   "source": [
    "## Pivoting columns\n",
    "\n",
    "What if we wanted to take the `quantity` column into account (i.e. for each purchase event a customer might buy MORE THAN ONE of a given product)?\n",
    "\n",
    "One way to analyze this is to use the `.pivot()` method.  `.pivot()` *roughly* \"makes a column horizontal\".  More precisely, it constructs a new table where the column names are taken from column *values* in the old table.\n",
    "\n",
    "To make sense of this we always need to start with a `.groupBy()` and end with an aggregation.\n",
    "It's easier seen than said:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accessible-xerox",
   "metadata": {},
   "outputs": [],
   "source": [
    "product_quantity = purchases_df.groupBy('customer_name').pivot('product_name').sum('quantity')\n",
    "product_quantity.toPandas().head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "controlling-orbit",
   "metadata": {},
   "source": [
    "Look at all of those `NaN` (not a number).  In this context that means that the customer never bought that particular product.  Let's fill those in with zeros:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "whole-airplane",
   "metadata": {},
   "outputs": [],
   "source": [
    "product_quantity = product_quantity.na.fill(0)\n",
    "product_quantity.toPandas().head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sorted-indie",
   "metadata": {},
   "source": [
    "Let's say we wanted to compute the average number of products purchased over all customers?  Let's start by getting a list of products:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "billion-cooperation",
   "metadata": {},
   "outputs": [],
   "source": [
    "products = product_quantity.columns[1:]\n",
    "products"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "copyrighted-america",
   "metadata": {},
   "source": [
    "It is easy to get averages by hand over a couple of products:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "meaning-parking",
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_quantity = product_quantity.groupBy().avg('apples', 'bananas')\n",
    "avg_quantity.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "skilled-livestock",
   "metadata": {},
   "source": [
    "If we want to compute averages for ALL products then we need to use a specify Python syntax.  Recall that we have a list of products in `products`.  We can \"unpack\" this list to be the arguments of a function by using the `*` operator:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "novel-rebound",
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_quantity_all = product_quantity.groupBy().avg(*products)\n",
    "avg_quantity_all.toPandas().head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "written-progress",
   "metadata": {},
   "source": [
    "## 2. UDFs and Windowing\n",
    "\n",
    "User-defined functions are very useful when performing computations on DataFrames.  These are similar in spirit to the lambdas that we often used when computing on RDDs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "extensive-income",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as fn\n",
    "from pyspark.sql.types import DoubleType\n",
    "\n",
    "# define the function itself\n",
    "def amount_spent(quantity, price):\n",
    "    return quantity*price\n",
    "\n",
    "# convert it to a UDF\n",
    "amount_spent_udf = fn.udf(amount_spent, DoubleType())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nuclear-postcard",
   "metadata": {},
   "source": [
    "Now create a new column named `amount_spent` where the values are computed using the UDF:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "frequent-gardening",
   "metadata": {},
   "outputs": [],
   "source": [
    "purchases_df = purchases_df.withColumn('amount_spent', amount_spent_udf(fn.col('quantity'), fn.col('price')))\n",
    "purchases_df.limit(5).toPandas().head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fifty-memorial",
   "metadata": {},
   "source": [
    "### Windowing\n",
    "\n",
    "Windowing is the way to aggregate a row with neighboring rows to produce interesting statistics.  For example, imagine answering questions like \"average spend over last 5 visits\".\n",
    "\n",
    "Let's just do a simple example:  cumulative historical spend"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "binary-anxiety",
   "metadata": {},
   "source": [
    "We can make this example more interesting.  Above we were computing spend per *visit*.  Very often it is interesting to answer questions about buckets of time (e.g. weekly spend).\n",
    "\n",
    "Just like we did for RDDs, we can use our old friend `datetime` to perform time analysis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "experimental-assembly",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Window\n",
    "\n",
    "#start by defining the window over which computations will be performed\n",
    "window = Window.partitionBy('customer_name').orderBy('date',).rowsBetween(Window.unboundedPreceding, 0)\n",
    "\n",
    "#now apply the window aggregation to compute a new column `cumulative_spend`\n",
    "purchases_df = purchases_df.withColumn('cumulative_spend', fn.sum(fn.col('amount_spent')).over(window))\n",
    "\n",
    "purchases_df.limit(20).toPandas().head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aquatic-software",
   "metadata": {},
   "outputs": [],
   "source": [
    "#start by defining the window over which computations will be performed\n",
    "window = Window.partitionBy('customer_name').orderBy('date',).rowsBetween(-2, 0)\n",
    "\n",
    "#now apply the window aggregation to compute a new column `cumulative_spend`\n",
    "purchases_df = purchases_df.withColumn('cumulative_spend_3', fn.sum(fn.col('amount_spent')).over(window))\n",
    "\n",
    "purchases_df.limit(20).toPandas().head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "urban-tower",
   "metadata": {},
   "outputs": [],
   "source": [
    "#start by creating a UDF that converts the date string to a datetime object\n",
    "from datetime import datetime\n",
    "from pyspark.sql.types import DateType\n",
    "\n",
    "def parse_date(datestr):\n",
    "    return datetime.strptime(datestr, '%Y-%m-%d')\n",
    "\n",
    "string_to_datetime = fn.udf(parse_date, DateType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "featured-maria",
   "metadata": {},
   "outputs": [],
   "source": [
    "purchases_df = purchases_df.withColumn('datetime', string_to_datetime(fn.col('date')))\n",
    "purchases_df = purchases_df.drop('date')\n",
    "purchases_df.limit(10).toPandas().head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "driven-stage",
   "metadata": {},
   "source": [
    "Let's add a `weekofyear` column so that we can aggregate by the week:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sitting-nickname",
   "metadata": {},
   "outputs": [],
   "source": [
    "purchases_df = purchases_df.withColumn('weekofyear', fn.weekofyear(fn.col('datetime')))\n",
    "purchases_df.limit(10).toPandas().head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cleared-table",
   "metadata": {},
   "source": [
    "Now aggregating by the week is easy!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "frequent-crest",
   "metadata": {},
   "outputs": [],
   "source": [
    "purchases_df.groupBy('customer_name', 'weekofyear').sum('amount_spent').orderBy('customer_name', 'weekofyear').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "comparative-olive",
   "metadata": {},
   "source": [
    "### Save dataframe to disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "victorian-million",
   "metadata": {},
   "outputs": [],
   "source": [
    "purchases_df.write.csv('./purchases_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "specified-ebony",
   "metadata": {},
   "outputs": [],
   "source": [
    "# parquet is very popular, and much more efficient than csv\n",
    "purchases_df.write.parquet('./purchases_df.parquet')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
