{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#python notebook for a 3-layer neural network on the MNIST dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.data as Data\n",
    "\n",
    "import numpy as np\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "\n",
    "    def __init__(self, learning_rate):\n",
    "        #call the base class's initialisation\n",
    "        super().__init__()\n",
    "        \n",
    "        #learning rate\n",
    "        self.lr = learning_rate\n",
    "        \n",
    "        #define the layers and their sizes\n",
    "        self.input_layer = nn.Linear(784, 640, bias=False)\n",
    "        self.hidden_layer = nn.Linear(640, 10, bias=False)\n",
    "        \n",
    "        #define activation function\n",
    "        self.activation = nn.Sigmoid()\n",
    "        \n",
    "        #create error/loss function, crossEntropy will include softmax tranformation\n",
    "        self.error_function = nn.CrossEntropyLoss()\n",
    "        \n",
    "        #create optimiser, using simple stochastic gradient descent\n",
    "        self.optimizer = torch.optim.SGD(self.parameters(), self.lr)\n",
    "\n",
    "    \n",
    "    def forward(self, inputs_list):\n",
    "        \n",
    "        #combine input layer signals into hidden layer\n",
    "        hidden_inputs = self.input_layer(inputs_list)\n",
    "        #apply sigmiod activation function\n",
    "        hidden_outputs = self.activation(hidden_inputs)\n",
    "        \n",
    "        #combine hidden layer signals into output layer\n",
    "        final_inputs = self.hidden_layer(hidden_outputs)\n",
    "        #apply sigmiod activation function\n",
    "        final_outputs = final_inputs\n",
    "        \n",
    "        return final_outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# learning rate\n",
    "learning_rate = 0.1\n",
    "\n",
    "# create instance of neural network\n",
    "n = NeuralNetwork(learning_rate)\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "n = n.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://pjreddie.com/projects/mnist-in-csv/\n",
    "#!wget https://pjreddie.com/media/files/mnist_train.csv\n",
    "#!wget https://pjreddie.com/media/files/mnist_test.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the mnist training data CSV file into np array\n",
    "train_data = np.genfromtxt(\"mnist_data/mnist_train.csv\", delimiter=',', dtype=np.float32)\n",
    "#784 columns of training data, normalized\n",
    "x_train = train_data[:,1:]\n",
    "x_train = x_train / 255.0\n",
    "y_train = train_data[:,0]\n",
    "\n",
    "test_data = np.genfromtxt(\"mnist_data/mnist_test.csv\", delimiter=',', dtype=np.float32)\n",
    "#test data - 784 columns, normalized\n",
    "x_test = test_data[:,1:]\n",
    "x_test = x_test / 255.0\n",
    "y_test = test_data[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch_x_train = torch.from_numpy(x_train)\n",
    "torch_y_train = torch.from_numpy(y_train).type(torch.LongTensor) \n",
    "torch_x_test = torch.from_numpy(x_test)\n",
    "torch_y_test = torch.from_numpy(y_test).type(torch.LongTensor) \n",
    "\n",
    "train_dataset = Data.TensorDataset(torch_x_train, torch_y_train)\n",
    "test_dataset = Data.TensorDataset(torch_x_test, torch_y_test)\n",
    "\n",
    "train_loader = Data.DataLoader(dataset=train_dataset, batch_size=100, num_workers=2)\n",
    "test_loader = Data.DataLoader(dataset=test_dataset, batch_size=100, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[001/030] 6.37 sec(s) Train Acc: 0.685667 Loss: 0.010596 | Val Acc: 0.843600 loss: 0.005146\n",
      "[002/030] 2.67 sec(s) Train Acc: 0.881850 Loss: 0.004227 | Val Acc: 0.884500 loss: 0.003822\n",
      "[003/030] 2.65 sec(s) Train Acc: 0.896650 Loss: 0.003590 | Val Acc: 0.900200 loss: 0.003387\n",
      "[004/030] 2.65 sec(s) Train Acc: 0.903367 Loss: 0.003328 | Val Acc: 0.908100 loss: 0.003178\n",
      "[005/030] 2.54 sec(s) Train Acc: 0.907533 Loss: 0.003174 | Val Acc: 0.910200 loss: 0.003053\n",
      "[006/030] 2.74 sec(s) Train Acc: 0.911250 Loss: 0.003067 | Val Acc: 0.912600 loss: 0.002968\n",
      "[007/030] 2.83 sec(s) Train Acc: 0.913550 Loss: 0.002984 | Val Acc: 0.914200 loss: 0.002902\n",
      "[008/030] 3.27 sec(s) Train Acc: 0.915900 Loss: 0.002915 | Val Acc: 0.916400 loss: 0.002847\n",
      "[009/030] 2.86 sec(s) Train Acc: 0.918183 Loss: 0.002853 | Val Acc: 0.917700 loss: 0.002797\n",
      "[010/030] 3.17 sec(s) Train Acc: 0.919883 Loss: 0.002795 | Val Acc: 0.920400 loss: 0.002748\n",
      "[011/030] 2.84 sec(s) Train Acc: 0.921400 Loss: 0.002739 | Val Acc: 0.921800 loss: 0.002700\n",
      "[012/030] 3.02 sec(s) Train Acc: 0.923267 Loss: 0.002683 | Val Acc: 0.922200 loss: 0.002651\n",
      "[013/030] 3.06 sec(s) Train Acc: 0.924850 Loss: 0.002626 | Val Acc: 0.923800 loss: 0.002599\n",
      "[014/030] 3.01 sec(s) Train Acc: 0.926717 Loss: 0.002568 | Val Acc: 0.925300 loss: 0.002546\n",
      "[015/030] 3.07 sec(s) Train Acc: 0.928533 Loss: 0.002508 | Val Acc: 0.926600 loss: 0.002490\n",
      "[016/030] 2.96 sec(s) Train Acc: 0.930317 Loss: 0.002447 | Val Acc: 0.928600 loss: 0.002433\n",
      "[017/030] 2.96 sec(s) Train Acc: 0.931817 Loss: 0.002386 | Val Acc: 0.930500 loss: 0.002376\n",
      "[018/030] 3.21 sec(s) Train Acc: 0.933750 Loss: 0.002325 | Val Acc: 0.932200 loss: 0.002318\n",
      "[019/030] 3.14 sec(s) Train Acc: 0.935917 Loss: 0.002264 | Val Acc: 0.934900 loss: 0.002261\n",
      "[020/030] 3.02 sec(s) Train Acc: 0.937750 Loss: 0.002204 | Val Acc: 0.936100 loss: 0.002205\n",
      "[021/030] 3.05 sec(s) Train Acc: 0.939500 Loss: 0.002145 | Val Acc: 0.937600 loss: 0.002150\n",
      "[022/030] 2.99 sec(s) Train Acc: 0.940950 Loss: 0.002088 | Val Acc: 0.939200 loss: 0.002097\n",
      "[023/030] 3.14 sec(s) Train Acc: 0.942667 Loss: 0.002032 | Val Acc: 0.940400 loss: 0.002045\n",
      "[024/030] 2.98 sec(s) Train Acc: 0.944450 Loss: 0.001978 | Val Acc: 0.941500 loss: 0.001996\n",
      "[025/030] 3.04 sec(s) Train Acc: 0.945933 Loss: 0.001927 | Val Acc: 0.942900 loss: 0.001948\n",
      "[026/030] 3.11 sec(s) Train Acc: 0.947400 Loss: 0.001876 | Val Acc: 0.944400 loss: 0.001902\n",
      "[027/030] 3.16 sec(s) Train Acc: 0.948483 Loss: 0.001828 | Val Acc: 0.945100 loss: 0.001859\n",
      "[028/030] 3.12 sec(s) Train Acc: 0.950033 Loss: 0.001781 | Val Acc: 0.945800 loss: 0.001817\n",
      "[029/030] 3.09 sec(s) Train Acc: 0.951283 Loss: 0.001737 | Val Acc: 0.946900 loss: 0.001777\n",
      "[030/030] 3.08 sec(s) Train Acc: 0.952500 Loss: 0.001693 | Val Acc: 0.948100 loss: 0.001738\n"
     ]
    }
   ],
   "source": [
    "epochs = 30\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    epoch_start_time = time.time()\n",
    "    train_acc = 0.0\n",
    "    train_loss = 0.0\n",
    "    test_acc = 0.0\n",
    "    test_loss = 0.0\n",
    "\n",
    "    n.train()\n",
    "    for i, data in enumerate(train_loader):\n",
    "        inputs = data[0].to(device)\n",
    "        labels = data[1].to(device)\n",
    "        \n",
    "        n.optimizer.zero_grad()\n",
    "        train_predicted = n.forward(inputs)\n",
    "        batch_loss = n.error_function(train_predicted, labels)\n",
    "        batch_loss.backward()\n",
    "        n.optimizer.step()\n",
    "\n",
    "        train_acc += np.sum(np.argmax(train_predicted.cpu().data.numpy(), axis=1) == data[1].numpy())\n",
    "        train_loss += batch_loss.item()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        n.eval()\n",
    "        for i, data in enumerate(test_loader):\n",
    "            inputs = data[0].to(device)\n",
    "            labels = data[1].to(device)\n",
    "            \n",
    "            test_predicted = n.forward(inputs)\n",
    "            batch_loss = n.error_function(test_predicted, labels)\n",
    "\n",
    "            test_acc += np.sum(np.argmax(test_predicted.cpu().data.numpy(), axis=1) == data[1].numpy())\n",
    "            test_loss += batch_loss.item()\n",
    "\n",
    "        #print result for each epoch\n",
    "        print('[%03d/%03d] %2.2f sec(s) Train Acc: %3.6f Loss: %3.6f | Val Acc: %3.6f loss: %3.6f' % \\\n",
    "            (epoch + 1, epochs, time.time()-epoch_start_time, \\\n",
    "             train_acc/train_dataset.__len__(), train_loss/train_dataset.__len__(), test_acc/test_dataset.__len__(), test_loss/test_dataset.__len__()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
