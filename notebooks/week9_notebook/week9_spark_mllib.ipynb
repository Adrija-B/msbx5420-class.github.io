{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "organized-halloween",
   "metadata": {},
   "source": [
    "# Introduction to Spark MLlib "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prompt-mathematics",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.master('local[4]').appName('spark_mllib').getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bibliographic-novel",
   "metadata": {},
   "source": [
    "## Load and transform the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "early-bangladesh",
   "metadata": {},
   "source": [
    "Next, we load the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "united-county",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "births = spark.read.options(inferSchema = True).csv('births_train.csv.gz', header = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "occasional-tract",
   "metadata": {},
   "source": [
    "Our goal is to predict whether the `'INFANT_ALIVE_AT_REPORT'` is either 1 or 0. Thus, we will drop all of the features that relate to the infant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acute-algorithm",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "selected_features = [\n",
    "    'INFANT_ALIVE_AT_REPORT', \n",
    "    'BIRTH_PLACE', \n",
    "    'MOTHER_AGE_YEARS', \n",
    "    'FATHER_COMBINED_AGE', \n",
    "    'CIG_BEFORE', \n",
    "    'CIG_1_TRI', \n",
    "    'CIG_2_TRI', \n",
    "    'CIG_3_TRI', \n",
    "    'MOTHER_HEIGHT_IN', \n",
    "    'MOTHER_PRE_WEIGHT', \n",
    "    'MOTHER_DELIVERY_WEIGHT', \n",
    "    'MOTHER_WEIGHT_GAIN', \n",
    "    'DIABETES_PRE', \n",
    "    'DIABETES_GEST', \n",
    "    'HYP_TENS_PRE', \n",
    "    'HYP_TENS_GEST', \n",
    "    'PREV_BIRTH_PRETERM'\n",
    "]\n",
    "\n",
    "births_trimmed = births.select(selected_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pointed-salem",
   "metadata": {},
   "source": [
    "Specify the recoding methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mature-lebanon",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as fn\n",
    "import pyspark.sql.types as types\n",
    "\n",
    "recode_dictionary = {'YNU': {'Y': 1, 'N': 0,'U': 0}}\n",
    "\n",
    "def recode(col, key):        \n",
    "    return recode_dictionary[key][col] \n",
    "\n",
    "def correct_cig(feat):\n",
    "    return fn.when(fn.col(feat) != 99, fn.col(feat)).otherwise(0)\n",
    "\n",
    "rec_integer = fn.udf(recode, types.IntegerType())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "structured-september",
   "metadata": {},
   "source": [
    "Correct the features related to the number of smoked cigarettes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "coupled-contractor",
   "metadata": {},
   "outputs": [],
   "source": [
    "births_transformed = births_trimmed \\\n",
    "    .withColumn('CIG_BEFORE', correct_cig('CIG_BEFORE'))\\\n",
    "    .withColumn('CIG_1_TRI', correct_cig('CIG_1_TRI'))\\\n",
    "    .withColumn('CIG_2_TRI', correct_cig('CIG_2_TRI'))\\\n",
    "    .withColumn('CIG_3_TRI', correct_cig('CIG_3_TRI'))\n",
    "births_transformed.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "threaded-penguin",
   "metadata": {},
   "source": [
    "Figure out which Yes/No/Unknown features are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "three-receptor",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = [(col.name, col.dataType) for col in births_trimmed.schema]\n",
    "\n",
    "YNU_cols = []\n",
    "\n",
    "for i, s in enumerate(cols):\n",
    "    if s[1] == types.StringType():\n",
    "        dis = births.select(s[0]).distinct().rdd.map(lambda row: row[0]).collect()\n",
    "\n",
    "        if 'Y' in dis:\n",
    "            YNU_cols.append(s[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "outdoor-transition",
   "metadata": {},
   "source": [
    "DataFrames can transform the features *in bulk* while selecting features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "brilliant-intermediate",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "births.select(['INFANT_NICU_ADMISSION', rec_integer('INFANT_NICU_ADMISSION', fn.lit('YNU')).alias('INFANT_NICU_ADMISSION_RECODE')]).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "advisory-inside",
   "metadata": {},
   "source": [
    "Transform all the `YNU_cols` in one using a list of transformations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "analyzed-bryan",
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in births_transformed.columns:\n",
    "    if col in YNU_cols:\n",
    "        births_transformed = births_transformed.withColumn(col, rec_integer(col, fn.lit('YNU')).alias(col))\n",
    "\n",
    "births_transformed.select(YNU_cols).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "secondary-supplier",
   "metadata": {},
   "source": [
    "## Get to know your data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "contemporary-memory",
   "metadata": {},
   "source": [
    "### Descriptive statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eleven-finance",
   "metadata": {},
   "source": [
    "We will use the `colStats(...)` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "confident-orchestra",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import pyspark.mllib.stat as st\n",
    "import numpy as np\n",
    "\n",
    "numeric_cols = ['MOTHER_AGE_YEARS','FATHER_COMBINED_AGE',\n",
    "                'CIG_BEFORE','CIG_1_TRI','CIG_2_TRI','CIG_3_TRI',\n",
    "                'MOTHER_HEIGHT_IN','MOTHER_PRE_WEIGHT',\n",
    "                'MOTHER_DELIVERY_WEIGHT','MOTHER_WEIGHT_GAIN']\n",
    "\n",
    "numeric_rdd = births_transformed.select(numeric_cols).rdd.map(lambda row: [e for e in row])\n",
    "\n",
    "mllib_stats = st.Statistics.colStats(numeric_rdd)\n",
    "\n",
    "for col, m, v in zip(numeric_cols, mllib_stats.mean(), mllib_stats.variance()):\n",
    "    print('{0}: \\t{1:.2f} \\t {2:.2f}'.format(col, m, np.sqrt(v)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "corresponding-substitute",
   "metadata": {},
   "source": [
    "For the categorical variables we will calculate the frequencies of their values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sacred-brave",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "categorical_cols = [e for e in births_transformed.columns if e not in numeric_cols]\n",
    "\n",
    "categorical_rdd = births_transformed.select(categorical_cols).rdd.map(lambda row: [e for e in row])\n",
    "            \n",
    "for i, col in enumerate(categorical_cols):\n",
    "    feq = categorical_rdd.groupBy(lambda row: row[i]).mapValues(lambda x: len(x))   \n",
    "    print(col, sorted(feq.collect(), key=lambda x: x[1], reverse=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "younger-cookie",
   "metadata": {},
   "source": [
    "### Correlations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "noble-defendant",
   "metadata": {},
   "source": [
    "Correlations between our features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dedicated-departure",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "corrs = st.Statistics.corr(numeric_rdd)\n",
    "print(corrs)\n",
    "\n",
    "for i, ele in enumerate(corrs > 0.5):\n",
    "    correlated = []\n",
    "    for j, e in enumerate(ele):\n",
    "        if e == True and j != i:\n",
    "            correlated.append((numeric_cols[j], corrs[i][j])) \n",
    "    \n",
    "    if len(correlated) > 0:\n",
    "        for p in correlated:\n",
    "            print('{0}-to-{1}: {2:.2f}'.format(numeric_cols[i], p[0], p[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "monetary-deadline",
   "metadata": {},
   "source": [
    "We can drop most of highly correlated features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fundamental-eagle",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "features_to_keep = [\n",
    "    'INFANT_ALIVE_AT_REPORT', \n",
    "    'BIRTH_PLACE', \n",
    "    'MOTHER_AGE_YEARS', \n",
    "    'FATHER_COMBINED_AGE', \n",
    "    'CIG_1_TRI', \n",
    "    'MOTHER_HEIGHT_IN', \n",
    "    'MOTHER_PRE_WEIGHT', \n",
    "    'DIABETES_PRE', \n",
    "    'DIABETES_GEST', \n",
    "    'HYP_TENS_PRE', \n",
    "    'HYP_TENS_GEST', \n",
    "    'PREV_BIRTH_PRETERM'\n",
    "]\n",
    "\n",
    "births_transformed = births_transformed.select(features_to_keep)\n",
    "births_transformed.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "preliminary-gibraltar",
   "metadata": {},
   "source": [
    "### Statistical testing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "likely-bottle",
   "metadata": {},
   "source": [
    "Run a Chi-square test to determine if there are significant differences for categorical variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stable-spanking",
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "naval-pricing",
   "metadata": {},
   "outputs": [],
   "source": [
    "feq = births_transformed.groupBy('INFANT_ALIVE_AT_REPORT').pivot(categorical_cols[1]).count()\n",
    "feq.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "informational-words",
   "metadata": {},
   "outputs": [],
   "source": [
    "flat_feq = feq.rdd.map(lambda row: (row[1:])).flatMap(lambda row: [0 if e == None else e for e in row]).collect()\n",
    "flat_feq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rural-backing",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.mllib.linalg as ln\n",
    "mat_feq = ln.Matrices.dense(8, 2, flat_feq)\n",
    "mat_feq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "contrary-disclaimer",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import pyspark.mllib.linalg as ln\n",
    "\n",
    "for cat in categorical_cols[1:]:\n",
    "    feq = births_transformed.groupBy('INFANT_ALIVE_AT_REPORT').pivot(cat).count()\n",
    "\n",
    "    flat_feq = feq.rdd.map(lambda row: (row[1:])).flatMap(lambda row: [0 if e == None else e for e in row]).collect()\n",
    "\n",
    "    row_length = len(feq.collect()[0]) - 1\n",
    "    mat_feq = ln.Matrices.dense(row_length, 2, flat_feq)\n",
    "    \n",
    "    test = st.Statistics.chiSqTest(mat_feq)\n",
    "    print(cat, round(test.pValue, 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tested-porcelain",
   "metadata": {},
   "source": [
    "## Create the final dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "smart-watch",
   "metadata": {},
   "source": [
    "### Create an RDD of `LabeledPoint`\n",
    "\n",
    "We will use a hashing trick to encode the `'BIRTH_PLACE'` feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "interesting-little",
   "metadata": {},
   "outputs": [],
   "source": [
    "births_transformed.show()\n",
    "births_transformed.select('BIRTH_PLACE').distinct().show()\n",
    "births_transformed.rdd.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rocky-murder",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import pyspark.mllib.feature as ft\n",
    "import pyspark.mllib.regression as reg\n",
    "\n",
    "hashing = ft.HashingTF(7)\n",
    "\n",
    "births_hashed = births_transformed.rdd.map(lambda row: [list(hashing.transform(str(row[1])).toArray()) \n",
    "                                                            if col == 'BIRTH_PLACE' else row[i] \n",
    "                                                        for i, col in enumerate(features_to_keep)])\n",
    "\n",
    "births_hashed.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "round-blair",
   "metadata": {},
   "outputs": [],
   "source": [
    "births_hashed_lists = births_hashed.map(lambda row: [[e] if type(e) == int else e for e in row])\n",
    "print(births_hashed_lists.take(5))\n",
    "births_hashed_all = births_hashed_lists.map(lambda row: [item for sublist in row for item in sublist])\n",
    "print(births_hashed_all.take(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "based-pharmacy",
   "metadata": {},
   "outputs": [],
   "source": [
    "births_hashed_final = births_hashed_all.map(lambda row: reg.LabeledPoint(row[0], ln.Vectors.dense(row[1:])))\n",
    "births_hashed_final.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "divided-ultimate",
   "metadata": {},
   "source": [
    "### Split into training and testing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "prescribed-activity",
   "metadata": {},
   "source": [
    "Before we move to the modeling stage, we need to split our dataset into two sets: one training set and one testing set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "residential-attribute",
   "metadata": {},
   "outputs": [],
   "source": [
    "births_train, births_test = births_hashed_final.randomSplit([0.7, 0.3], seed = 200)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "skilled-treaty",
   "metadata": {},
   "source": [
    "## Predicting infant survival"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "absent-cooler",
   "metadata": {},
   "source": [
    "### Logistic regression in Spark\n",
    "\n",
    "MLLib used to provide a logistic regression model estimated using a stochastic gradient descent (SGD) algorithm. This model has been deprecated in Spark 2.0 in favor of the `LogisticRegressionWithLBFGS` model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "suspected-interpretation",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.classification import LogisticRegressionWithLBFGS\n",
    "\n",
    "LR_Model = LogisticRegressionWithLBFGS.train(births_train, iterations=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "together-alignment",
   "metadata": {},
   "source": [
    "Let's now use the model to predict the classes for our testing set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cloudy-joining",
   "metadata": {},
   "outputs": [],
   "source": [
    "LR_truth = births_test.map(lambda row: row.label).zipWithIndex().map(lambda row: (row[1], row[0]))\n",
    "LR_truth.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "downtown-underwear",
   "metadata": {},
   "outputs": [],
   "source": [
    "LR_predicted = LR_Model.predict(births_test.map(lambda row: row.features)).zipWithIndex().map(lambda row: (row[1], row[0] * 1.0))\n",
    "LR_predicted.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "jewish-softball",
   "metadata": {},
   "outputs": [],
   "source": [
    "LR_results = LR_truth.join(LR_predicted).map(lambda row: row[1])\n",
    "LR_results.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "intelligent-madonna",
   "metadata": {},
   "source": [
    "Let's check how well or how bad our model performed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "super-vampire",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import pyspark.mllib.evaluation as ev\n",
    "LR_evaluation = ev.BinaryClassificationMetrics(LR_results)\n",
    "\n",
    "print('Area under PR: {0:.2f}'.format(LR_evaluation.areaUnderPR))\n",
    "print('Area under ROC: {0:.2f}'.format(LR_evaluation.areaUnderROC))\n",
    "LR_evaluation.unpersist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pursuant-logging",
   "metadata": {},
   "source": [
    "### Random Forest in Spark\n",
    "\n",
    "We are now ready to build the random forest model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "spiritual-camel",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.tree import RandomForest\n",
    "\n",
    "RF_model = RandomForest.trainClassifier(data=births_train, \n",
    "                                        numClasses=2, \n",
    "                                        categoricalFeaturesInfo={}, \n",
    "                                        numTrees=6,  \n",
    "                                        featureSubsetStrategy='all',\n",
    "                                        seed=666)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "distinguished-breakfast",
   "metadata": {},
   "source": [
    "Let's see how well our model did."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "humanitarian-simpson",
   "metadata": {},
   "outputs": [],
   "source": [
    "RF_truth = births_test.map(lambda row: row.label).zipWithIndex().map(lambda row: (row[1], row[0]))\n",
    "RF_truth.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "serious-jumping",
   "metadata": {},
   "outputs": [],
   "source": [
    "RF_predicted = RF_model.predict(births_test.map(lambda row: row.features)).zipWithIndex().map(lambda row: (row[1], row[0] * 1.0))\n",
    "RF_predicted.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "reasonable-catch",
   "metadata": {},
   "outputs": [],
   "source": [
    "RF_results = RF_truth.join(RF_predicted).map(lambda row: row[1])\n",
    "RF_results.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "concerned-surname",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "RF_evaluation = ev.BinaryClassificationMetrics(RF_results)\n",
    "\n",
    "print('Area under PR: {0:.2f}'.format(RF_evaluation.areaUnderPR))\n",
    "print('Area under ROC: {0:.2f}'.format(RF_evaluation.areaUnderROC))\n",
    "RF_evaluation.unpersist()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
