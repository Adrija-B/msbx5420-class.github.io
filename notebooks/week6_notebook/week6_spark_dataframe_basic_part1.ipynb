{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "colored-veteran",
   "metadata": {},
   "source": [
    "# Spark DataFrames and SQL Basics Part I"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beautiful-watson",
   "metadata": {},
   "source": [
    "## 1. Spark DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "optimum-supervisor",
   "metadata": {},
   "outputs": [],
   "source": [
    "#for DataFrame we use a SparkSession instead of a SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# SparkSession uses the \"builder\" syntax\n",
    "spark = SparkSession.builder.master('local').appName('Week 6').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "opponent-jewelry",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "#\"row\" storage\n",
    "row_table = [{\"col1\": 3, \"col2\": 6.23, \"col3\": 8.51},\n",
    "             {\"col1\": 5, \"col2\": 4.2, \"col3\": 7.45},\n",
    "             {\"col1\": 4, \"col2\": 6.8, \"col3\": 9.2},\n",
    "             {\"col1\": 6, \"col2\": 7.8, \"col3\": 10.5}]\n",
    "\n",
    "pandas_df = pd.DataFrame(row_table)\n",
    "pandas_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "civil-springfield",
   "metadata": {},
   "source": [
    "These examples are very similar in spirit to `sc.parallelize` for RDDs.  We are pushing small amounts of data up into Spark, usually for practice purposes.\n",
    "\n",
    "### from list of dicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "respected-alpha",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "\n",
    "#spark defaults to row-based, so we have to feed it row-by-row\n",
    "df = spark.createDataFrame([Row(col1=3, col2=6.23, col3=8.51), Row(col1=5, col2=4.2, col3=7.45), Row(col1=4, col2=6.8, col3=9.2), Row(col1=6, col2=7.8, col3=10.5)])\n",
    "df.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "greater-verification",
   "metadata": {},
   "source": [
    "### from `pandas`\n",
    "\n",
    "We can also create from a `pandas` DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "armed-infrastructure",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.createDataFrame(pandas_df)\n",
    "df.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "herbal-pastor",
   "metadata": {},
   "source": [
    "### from list of tuples\n",
    "\n",
    "Another easy way to create a DataFrame is from a list of tuples, passing the column names explicitly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "numerical-terminal",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.createDataFrame([(3, 6.23, 8.51), (5, 4.2, 7.45), (4, 6.8, 9.2), (6, 7.8, 10.5)], [\"col1\", \"col2\", \"col3\"])\n",
    "df.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "weird-enlargement",
   "metadata": {},
   "source": [
    "### from RDD\n",
    "\n",
    "RDDs look very similar to a list of tuples, so it shouldn't surprise you that we can create a DataFrame from an RDD using almost exactly the same syntax."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "processed-equation",
   "metadata": {},
   "outputs": [],
   "source": [
    "#we can get the underlying SparkContext in order to play with RDDs\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unable-august",
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd = sc.parallelize([(3, 6.23, 8.51), (5, 4.2, 7.45), (4, 6.8, 9.2), (6, 7.8, 10.5)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "comparable-bermuda",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.createDataFrame(rdd, [\"col1\", \"col2\", \"col3\"])\n",
    "df.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "initial-grace",
   "metadata": {},
   "source": [
    "### to RDD\n",
    "\n",
    "We can also convert a DataFrame back to an RDD (but an RDD of `Row`s).\n",
    "\n",
    "`Row` and `Column` are classes out of which the `DataFrame` class is built."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "regulated-thought",
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd2 = df.rdd\n",
    "rdd2.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "homeless-kingston",
   "metadata": {},
   "source": [
    "### to `pandas`\n",
    "\n",
    "We can convert back to a `pandas` DataFrame (this brings all the data back to the driver, basically like a `.collect()`, so be careful)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "considered-toddler",
   "metadata": {},
   "outputs": [],
   "source": [
    "pandas_df2 = df.toPandas()\n",
    "pandas_df2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "standing-flesh",
   "metadata": {},
   "source": [
    "## Look at content: take() vs head() vs show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "spatial-hello",
   "metadata": {},
   "outputs": [],
   "source": [
    "#same as the method for RDD\n",
    "df.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wrong-deficit",
   "metadata": {},
   "outputs": [],
   "source": [
    "#same as the head() method in pandas\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "connected-saskatchewan",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cheap-movie",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.toPandas().head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "biological-consolidation",
   "metadata": {},
   "source": [
    "### Basic DataFrame Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "secret-cambodia",
   "metadata": {},
   "outputs": [],
   "source": [
    "#rename column\n",
    "df = df.withColumnRenamed(\"col3\", \"col3a\")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unable-alberta",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a new column\n",
    "df = df.withColumn(\"col3b\", df['col3a']/2)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "careful-share",
   "metadata": {},
   "outputs": [],
   "source": [
    "#filter data\n",
    "df2 = df.filter(df['col3a'] >= 8)\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "french-flesh",
   "metadata": {},
   "outputs": [],
   "source": [
    "#can also use where()\n",
    "df2 = df.where(df['col3a'] >= 8)\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "organized-costa",
   "metadata": {},
   "source": [
    "## 2. DataFrame Schemas\n",
    "\n",
    "In all of the examples above the schema was *inferred*.  Spark just looked at the data and decided what type it should be.  Spark might make a mistake."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "supported-edwards",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.createDataFrame([(3, 6.23, 8.51), (5, 4.2, 7.45), (4, 6.8, 9.2), (6, 7.8, 10.5)], [\"col1\", \"col2\", \"col3\"])\n",
    "#check the type of columns\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "continued-combine",
   "metadata": {},
   "outputs": [],
   "source": [
    "#call the schema of dataframe\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "checked-canadian",
   "metadata": {},
   "source": [
    "What if I intended for `col1` to not get very large?  It might be more space efficient for me to store it as a single `byte`.\n",
    "We can specify a *schema*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "small-virgin",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, ByteType, DoubleType\n",
    "\n",
    "#the True arguments specify whether or not the data is allowed to be missing (null)\n",
    "schema = StructType([StructField(\"col1\", ByteType(), True),\n",
    "                     StructField(\"col2\", DoubleType(), True),\n",
    "                     StructField(\"col3\", DoubleType(), True)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "optimum-variable",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_from_schema = spark.createDataFrame([(3, 6.23, 8.51), (5, 4.2, 7.45), (4, 6.8, 9.2), (6, 7.8, 10.5)], schema)\n",
    "df_from_schema.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "monetary-bleeding",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_from_schema.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "religious-devon",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_from_schema.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "secret-straight",
   "metadata": {},
   "source": [
    "## 3. DataFrame and SQL\n",
    "Let's create a DataFrame by generating the data.  In this case, we'll first create the `JsonRDD` RDD and then convert it into a DataFrame when we're reading `JsonRDD` using `spark.read.json`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "expired-pavilion",
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate our own JSON data \n",
    "JsonRDD = sc.parallelize((\"\"\" \n",
    "  { \"id\": \"123\",\n",
    "    \"name\": \"Katie\",\n",
    "    \"age\": 19,\n",
    "    \"score\": 33.56,\n",
    "    \"eyeColor\": \"brown\"\n",
    "  }\"\"\",\n",
    "   \"\"\"{\n",
    "    \"id\": \"234\",\n",
    "    \"name\": \"Michael\",\n",
    "    \"age\": 22,\n",
    "    \"score\": 41.21,\n",
    "    \"eyeColor\": \"green\"\n",
    "  }\"\"\", \n",
    "  \"\"\"{\n",
    "    \"id\": \"345\",\n",
    "    \"name\": \"Simone\",\n",
    "    \"age\": 23,\n",
    "    \"score\": 28.45,\n",
    "    \"eyeColor\": \"blue\"\n",
    "  }\"\"\",\n",
    "  \"\"\"{\n",
    "    \"id\": \"456\",\n",
    "    \"name\": \"Alan\",\n",
    "    \"age\": 20,\n",
    "    \"score\": 36.53,\n",
    "    \"eyeColor\": \"brown\"\n",
    "  }\"\"\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "italic-ebony",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create dataframe\n",
    "swimmersJSON = spark.read.json(JsonRDD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "floating-agreement",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a temporary table\n",
    "swimmersJSON.createOrReplaceTempView(\"swimmersJSON\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "national-breath",
   "metadata": {},
   "outputs": [],
   "source": [
    "swimmersJSON.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "marked-uncertainty",
   "metadata": {},
   "outputs": [],
   "source": [
    "#SQL Query\n",
    "spark.sql(\"select * from swimmersJSON\").collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "strange-passport",
   "metadata": {},
   "outputs": [],
   "source": [
    "#SQL Query\n",
    "spark.sql(\"select * from swimmersJSON where score > 30\").collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lasting-ivory",
   "metadata": {},
   "source": [
    "#### Inferring the Schema Using Reflection\n",
    "Note that Spark is inferring the schema using reflection; i.e. it automaticlaly determines the schema of the data based on reviewing the JSON data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "close-niagara",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the schema\n",
    "swimmersJSON.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "innocent-thesaurus",
   "metadata": {},
   "source": [
    "Notice that Spark was able to determine infer the schema. But now we want a different schema."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tough-youth",
   "metadata": {},
   "source": [
    "In this case, let's specify the schema for a `CSV` format text file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "grand-guyana",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "\n",
    "#generate our own CSV data \n",
    "stringCSVRDD = sc.parallelize([(123, 'Katie', 19, 33.56, 'brown'), (234, 'Michael', 22, 41.21, 'green'), (345, 'Simone', 23, 28.45, 'blue'), (456, 'Alan', 20, 36.53, 'brown')])\n",
    "\n",
    "# The schema is encoded in a string, using StructType we define the schema using various pyspark.sql.types\n",
    "schemaString = \"id name age eyeColor\"\n",
    "schema = StructType([\n",
    "    StructField(\"id\", LongType(), True),\n",
    "    StructField(\"name\", StringType(), True),\n",
    "    StructField(\"age\", LongType(), True),\n",
    "    StructField(\"score\", DoubleType(), True),\n",
    "    StructField(\"eyeColor\", StringType(), True)\n",
    "])\n",
    "\n",
    "#apply the schema to the RDD and create dataframe\n",
    "swimmers = spark.createDataFrame(stringCSVRDD, schema)\n",
    "\n",
    "#creates a temporary view from the dataframe\n",
    "swimmers.createOrReplaceTempView(\"swimmers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "excessive-visit",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print the schema\n",
    "swimmers.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "funky-month",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"select * from swimmers\").collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "liable-remains",
   "metadata": {},
   "source": [
    "## 4. Querying with SQL\n",
    "With DataFrames, you can start writing your queries using `Spark SQL`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "decreased-canon",
   "metadata": {},
   "outputs": [],
   "source": [
    "#execute SQL Query and return the data\n",
    "spark.sql(\"select * from swimmers\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "close-celebration",
   "metadata": {},
   "source": [
    "Let's get the row count:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "honest-saver",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get count of rows in SQL\n",
    "spark.sql(\"select count(*) from swimmers\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "atmospheric-racing",
   "metadata": {},
   "outputs": [],
   "source": [
    "#query id and age for swimmers with age = 22 in SQL\n",
    "spark.sql(\"select id, age from swimmers where age = 22\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "arbitrary-coach",
   "metadata": {},
   "outputs": [],
   "source": [
    "#query name and eye color for swimmers with eye color starting with the letter 'b'\n",
    "spark.sql(\"select name, eyeColor from swimmers where eyeColor like 'b%'\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "joint-snowboard",
   "metadata": {},
   "source": [
    "## 5. Querying with the DataFrame API\n",
    "With DataFrames, you can start writing your queries using the DataFrame API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "arbitrary-control",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the values \n",
    "swimmers.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "narrow-frank",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using Databricks `display` command to view the data easier\n",
    "display(swimmers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "guilty-assignment",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get count of rows\n",
    "swimmers.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "molecular-administrator",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the id, age where age = 22\n",
    "swimmers.select(\"id\", \"age\").filter(\"age = 22\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "meaningful-priest",
   "metadata": {},
   "outputs": [],
   "source": [
    "#same, but with where()\n",
    "swimmers.select(\"id\", \"age\").where(\"age = 22\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nutritional-swift",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the name, eyeColor where eyeColor like 'b%'\n",
    "swimmers.select(\"name\", \"eyeColor\").filter(\"eyeColor like 'b%'\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acknowledged-patent",
   "metadata": {},
   "outputs": [],
   "source": [
    "#query id and age for swimmers with age = 22 via DataFrame API in another way\n",
    "swimmers.select(swimmers.id, swimmers.age).filter(swimmers.age == 22).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "engaging-coast",
   "metadata": {},
   "source": [
    "# Quick Exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "original-wireless",
   "metadata": {},
   "source": [
    "## 1. Obtain the highest score with SQL query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "several-prediction",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "liquid-westminster",
   "metadata": {},
   "source": [
    "## 2. Obtain players with scores over 30 and with brown eyes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "controversial-enhancement",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
