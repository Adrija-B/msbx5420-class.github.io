{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "# Spark Programming - Resilient Distributed Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Basic RDD Operations\n",
    "\n",
    "**Transformations** return new RDDs\n",
    "\n",
    "- `map`, `flatmap`\n",
    "- `reduceByKey`\n",
    "- `filter`\n",
    "\n",
    "**Actions** return values\n",
    "\n",
    "- `collect`\n",
    "- `reduce`\n",
    "- `take`\n",
    "- `count`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Everything starts with a `SparkContext`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "\n",
    "#from pyspark import SparkContext\n",
    "#sc = SparkContext.getOrCreate()\n",
    "\n",
    "#conf = SparkConf().setAppName(\"PySpark App\").setMaster(\"spark://spark-master:7077\")\n",
    "#sc = SparkContext(conf=conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.getOrCreate()\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the spark configuration\n",
    "sc.getConf().getAll()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do something to prove it works\n",
    "rdd = sc.parallelize(range(1000))\n",
    "rdd.takeSample(False, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd = sc.parallelize(range(0, 100, 1), 5)\n",
    "print(rdd.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "What does this look like?\n",
    "\n",
    "- `glom`: Returns an RDD list from each partition of an RDD.\n",
    "- `collect`: Returns a list from all elements of an RDD to the Driver Program."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in rdd.glom().collect():\n",
    "    print(x) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Use map and square each element of the RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#two transformations - A DAG with two tasks\n",
    "mappedRdd = (rdd.map(lambda x:x*x).filter(lambda x:x>1000))\n",
    "\n",
    "#one action\n",
    "print(mappedRdd.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mappedRdd.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Use reduce to sum the squares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reduced = mappedRdd.reduce(lambda x,y:x+y)\n",
    "print(reduced)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Map and Reduce"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### `map` and `flatMap`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a new RDD  - a list of lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd = sc.parallelize([ [2, 3, 4], [0, 1, 2, 3], [5, 6, 7, 8] ])\n",
    "rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "firstElement = rdd.map(lambda x: x[0])\n",
    "firstElement.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lastElement = rdd.map(lambda x: x[-1])\n",
    "lastElement.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lastElement = rdd.map(lambda x: x[-1:])\n",
    "lastElement.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Return a new RDD by first applying a function (the length of each string), then flatten."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = rdd.map(lambda x: range(len(x))).collect()\n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or I can flatten the results..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = rdd.flatMap(lambda x: range(len(x))).collect()\n",
    "z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or flatten the original results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd.flatMap(lambda x: x).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Reduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd = sc.parallelize(range(1000), 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd.reduce(lambda x,y: x+y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd = sc.parallelize([ [2, 3, 4], [0, 1, 2, 3], [5, 6, 7, 8] ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd.flatMap(lambda x: x).reduce(lambda x,y: x+y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    " ## 3. RDD with Key Value Pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd = sc.parallelize([(\"cat\", 1), (\"dog\", 1), (\"cat\", 2)])\n",
    "rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first = rdd.map(lambda x: x[0])\n",
    "first.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd.reduceByKey(lambda x,y: x+y).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd.groupByKey().mapValues(lambda x: sum(x)).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd = sc.parallelize([(\"cat\", 1), (\"dog\", 2), (\"cat\", 1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd.countByKey()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd.reduceByKey(lambda x,y: x+y).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#optional example - generate key value pair RDD from regular RDD\n",
    "simple_rdd = sc.parallelize([6, 3, 4, 53, 654, 2, 5, 8, 1 , 65, 66, 54])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key_value_rdd = simple_rdd.map(lambda x: (x % 2, x))\n",
    "key_value_rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key_value_rdd = simple_rdd.keyBy(lambda x: x % 2)\n",
    "key_value_rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key_value_rdd.countByKey()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "even_odd_summed_rdd = key_value_rdd.reduceByKey(lambda x,y: x+y)\n",
    "even_odd_summed_rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_rdd = key_value_rdd.groupByKey()\n",
    "grouped_rdd = grouped_rdd.mapValues(list)\n",
    "grouped_rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_rdd = simple_rdd.groupBy(lambda x: x % 2)\n",
    "print(grouped_rdd.collect())\n",
    "grouped_rdd = grouped_rdd.mapValues(list)\n",
    "grouped_rdd.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. RDD Partitions\n",
    "\n",
    "*Partitioning* is the process of distributing data across workers.  This allows workers to process in parallel.  The final results are then collated and combined.\n",
    "\n",
    "Under the hood, each worker machine is subdivided into \"executors\".    Often 1 executor = 1 core.\n",
    "\n",
    "You should have # partitions at LEAST equal to the total number of executors in your cluster, otherwise some executors will just sit idle.\n",
    "\n",
    "Each partition is processed sequentially on a single executor.  There is one *task* per element in the partition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#string concatenation\n",
    "strconcat = \"hello\" + \"there\"\n",
    "print(strconcat)\n",
    "strconcat = \"hello\" + \" \" + \"there\"\n",
    "print(strconcat)\n",
    "\n",
    "#converting numbers into strings\n",
    "numstr = str(5.6)\n",
    "print(numstr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#a simple function to combine numbers\n",
    "def combine_num(x, y):\n",
    "    return \"(\" + str(x) + \" \" + str(y) + \")\"\n",
    "\n",
    "combine_num(5, 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combine_num(5, combine_num(7, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combine_num(combine_num(5, 7), 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a simple RDD\n",
    "simple_rdd = sc.parallelize([6, 8, 2, 9, 10, 13, 7, 4])\n",
    "\n",
    "#let's reduce an RDD that has only a single partition\n",
    "simple_rdd.reduce(combine_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's partition the kid ages into 2 partitions\n",
    "simple_rdd2 = sc.parallelize([6, 8, 2, 9, 10, 13, 7, 4], 2)  # second argument specifies number of partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's reduce an RDD that has 2 partitions\n",
    "simple_rdd2.reduce(combine_num)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The associative and commutative rule of reduce\n",
    "\n",
    "The results is different depending on the number of partitions you specify. So in this case, if we divide the large problem into partitions, we can different results when having different partitions.  Because of this, `reduce` only works when the operation is *associative* (in the mathematical sense).\n",
    "\n",
    "For example, ((A + B) + C) = (A + (B + C)); but we can't guarantee ((A / B) / C) = (A / (B / C))\n",
    "\n",
    "This also won't be consistent run after run unless the combine operation is *commutative* (in the mathematical sense)\n",
    "\n",
    "For example, (A + B) = (B + A); but (A / B) is not necessarily equal to (B / A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So if the reducing function is not associative and commutative you will sometimes get wrong results depending how your data is partitioned.\n",
    "Let look at another example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "#if we divide the values by the sequence in the list, we expect to get 10\n",
    "simple_rdd = sc.parallelize([1, 2, 0.5, 0.1, 5, 0.2], 1)\n",
    "simple_rdd.reduce(lambda x, y: x / y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, if you were to partition the data into 3 partitions, the result will be wrong."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "simple_rdd2 = sc.parallelize([1, 2, 0.5, 0.1, 5, 0.2], 3)\n",
    "simple_rdd2.reduce(lambda x, y: x / y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. RDD LAZY Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#regular Python (operations on lists) is also LAZY (when programming in the functional style)\n",
    "data = [1.6, 2.4, 7.8, 4.6, 2.3]\n",
    "\n",
    "newdata_lazy = map(lambda x: x+1, data)\n",
    "newdata_lazy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#you can force the \"action\" by explicitly converting to a list\n",
    "newdata = list(newdata_lazy)\n",
    "newdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#let's look at this in spark\n",
    "data_rdd = sc.parallelize(data)\n",
    "#nothing has been done at this point\n",
    "newdata_rdd = data_rdd.map(lambda x: x+1)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform the .collect() action to bring the modified list back to driver\n",
    "newdata = newdata_rdd.collect()\n",
    "newdata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## 6. Task - Simple WordCount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordsList = ['dog', 'cat', 'cat', 'bird', 'dog', 'elephant', 'cat']\n",
    "wordsRDD = sc.parallelize(wordsList, 4)\n",
    "#check the type of wordsRDD\n",
    "print(type(wordsRDD))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#look at RDD partitions\n",
    "for x in wordsRDD.glom().collect():\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordPairs = wordsRDD.map(lambda x:(x,1))\n",
    "wordPairs.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Find the number of unique words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note that groupByKey requires no parameters\n",
    "wordsGrouped = wordPairs.groupByKey()\n",
    "for key, value in wordsGrouped.collect():\n",
    "    print ('{}: {}'.format(key, list(value)))\n",
    "wordsGrouped.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uniqueWords = wordsGrouped.keys().count()\n",
    "print(uniqueWords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Reduce to get the word count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note that reduceByKey takes in a function that accepts two values and returns a single value\n",
    "wordCounts = wordPairs.reduceByKey(lambda x,y: x+y)\n",
    "wordCounts.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "totalCount = (wordCounts.map(lambda x:x[1]).reduce(lambda x,y: x+y))\n",
    "average = totalCount / float(uniqueWords)\n",
    "print(totalCount)\n",
    "print(round(average, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
