{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "indonesian-beaver",
   "metadata": {},
   "source": [
    "## MSBX 5420 Assignment 3\n",
    "This assignment is about Spark Machine Learning and Spark Streaming. First two tasks focus on machine learning, and the third one combines machine learning and streaming analysis. We will use IMDB reviews data for the whole assignment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "quiet-heater",
   "metadata": {},
   "source": [
    "### Task 1 - Topic Modeling on Moive Reviews with Spark ML\n",
    "First of all, let's load the data. The data structure is very simple.One column is review text, and another column is the label of review sentiment (positive or negative). Same as exercise, we can load .csv.gz file directly from spark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "immune-southeast",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.master('local[4]').config(\"spark.executor.memory\", \"1g\").config(\"spark.driver.memory\", \"2g\").appName('spark_ml_imdb').getOrCreate()\n",
    "#for cluster - change kernel to PySpark\n",
    "#spark = SparkSession.builder.master('spark://spark-master:7077').appName('spark_ml_imdb').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cultural-precipitation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------+\n",
      "|              review|sentiment|\n",
      "+--------------------+---------+\n",
      "|One of the other ...| positive|\n",
      "|A wonderful littl...| positive|\n",
      "|I thought this wa...| positive|\n",
      "|Basically there's...| negative|\n",
      "|Petter Mattei's \"...| positive|\n",
      "|Probably my all-t...| positive|\n",
      "|I sure would like...| positive|\n",
      "|This show was an ...| negative|\n",
      "|Encouraged by the...| negative|\n",
      "|If you like origi...| positive|\n",
      "|Phil the Alien is...| negative|\n",
      "|I saw this movie ...| negative|\n",
      "|So im not a big f...| negative|\n",
      "|The cast played S...| negative|\n",
      "|This a fantastic ...| positive|\n",
      "|Kind of drawn in ...| negative|\n",
      "|Some films just s...| positive|\n",
      "|This movie made i...| negative|\n",
      "|I remember this f...| positive|\n",
      "|An awful film! It...| negative|\n",
      "+--------------------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "reviews = spark.read.options(inferSchema = True, multiLine = True, escape = '\\\"').csv('IMDB_Reviews.csv.gz', header=True)\n",
    "#for cluster\n",
    "#reviews = spark.read.options(inferSchema = True, multiLine = True, escape = '\\\"').csv('s3://msbx5420-spr22/IMDB_Reviews.csv.gz', header=True)\n",
    "reviews.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "furnished-variation",
   "metadata": {},
   "source": [
    "First, we should clean up the review texts. Besides those special characters we have tried to remove in exercise, here we also need to remove the html tags in the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "extra-church",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(review='One of the other reviewers has mentioned that after watching just 1 Oz episode you ll be hooked  They are right  as this is exactly what happened with me   The first thing that struck me about Oz was its brutality and unflinching scenes of violence  which set in right from the word GO  Trust me  this is not a show for the faint hearted or timid  This show pulls no punches with regards to drugs  sex or violence  Its is hardcore  in the classic use of the word   It is called OZ as that is the nickname given to the Oswald Maximum Security State Penitentary  It focuses mainly on Emerald City  an experimental section of the prison where all the cells have glass fronts and face inwards  so privacy is not high on the agenda  Em City is home to many Aryans  Muslims  gangstas  Latinos  Christians  Italians  Irish and more so scuffles  death stares  dodgy dealings and shady agreements are never far away   I would say the main appeal of the show is due to the fact that it goes where other shows wouldn t dare  Forget pretty pictures painted for mainstream audiences  forget charm  forget romance OZ doesn t mess around  The first episode I ever saw struck me as so nasty it was surreal  I couldn t say I was ready for it  but as I watched more  I developed a taste for Oz  and got accustomed to the high levels of graphic violence  Not just violence  but injustice  crooked guards who ll be sold out for a nickel  inmates who ll kill on order and get away with it  well mannered  middle class inmates being turned into prison bitches due to their lack of street skills or prison experience  Watching Oz  you may become comfortable with what is uncomfortable viewing thats if you can get in touch with your darker side ', sentiment='positive')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pyspark.sql.functions as fn\n",
    "import pyspark.ml.feature as ft\n",
    "\n",
    "#remove html tags in the text with regular expression\n",
    "reviews = reviews.withColumn('review', fn.regexp_replace(fn.col(\"review\"), '<[^>]+>', ' '))\n",
    "#remove special characters and line breaks in the text with regular expression\n",
    "reviews = reviews.withColumn('review', fn.regexp_replace(fn.col(\"review\"), '([^\\s\\w_]|_)+', ' ')).withColumn('review', fn.regexp_replace(fn.col(\"review\"), '[\\n\\r]', ' '))\n",
    "reviews.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fluid-thanksgiving",
   "metadata": {},
   "source": [
    "Now let's create tokenizer to start the data processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "large-compensation",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(review_tok=['one', 'of', 'the', 'other', 'reviewers', 'has', 'mentioned', 'that', 'after', 'watching', 'just', '1', 'oz', 'episode', 'you', 'll', 'be', 'hooked', 'they', 'are', 'right', 'as', 'this', 'is', 'exactly', 'what', 'happened', 'with', 'me', 'the', 'first', 'thing', 'that', 'struck', 'me', 'about', 'oz', 'was', 'its', 'brutality', 'and', 'unflinching', 'scenes', 'of', 'violence', 'which', 'set', 'in', 'right', 'from', 'the', 'word', 'go', 'trust', 'me', 'this', 'is', 'not', 'a', 'show', 'for', 'the', 'faint', 'hearted', 'or', 'timid', 'this', 'show', 'pulls', 'no', 'punches', 'with', 'regards', 'to', 'drugs', 'sex', 'or', 'violence', 'its', 'is', 'hardcore', 'in', 'the', 'classic', 'use', 'of', 'the', 'word', 'it', 'is', 'called', 'oz', 'as', 'that', 'is', 'the', 'nickname', 'given', 'to', 'the', 'oswald', 'maximum', 'security', 'state', 'penitentary', 'it', 'focuses', 'mainly', 'on', 'emerald', 'city', 'an', 'experimental', 'section', 'of', 'the', 'prison', 'where', 'all', 'the', 'cells', 'have', 'glass', 'fronts', 'and', 'face', 'inwards', 'so', 'privacy', 'is', 'not', 'high', 'on', 'the', 'agenda', 'em', 'city', 'is', 'home', 'to', 'many', 'aryans', 'muslims', 'gangstas', 'latinos', 'christians', 'italians', 'irish', 'and', 'more', 'so', 'scuffles', 'death', 'stares', 'dodgy', 'dealings', 'and', 'shady', 'agreements', 'are', 'never', 'far', 'away', 'i', 'would', 'say', 'the', 'main', 'appeal', 'of', 'the', 'show', 'is', 'due', 'to', 'the', 'fact', 'that', 'it', 'goes', 'where', 'other', 'shows', 'wouldn', 't', 'dare', 'forget', 'pretty', 'pictures', 'painted', 'for', 'mainstream', 'audiences', 'forget', 'charm', 'forget', 'romance', 'oz', 'doesn', 't', 'mess', 'around', 'the', 'first', 'episode', 'i', 'ever', 'saw', 'struck', 'me', 'as', 'so', 'nasty', 'it', 'was', 'surreal', 'i', 'couldn', 't', 'say', 'i', 'was', 'ready', 'for', 'it', 'but', 'as', 'i', 'watched', 'more', 'i', 'developed', 'a', 'taste', 'for', 'oz', 'and', 'got', 'accustomed', 'to', 'the', 'high', 'levels', 'of', 'graphic', 'violence', 'not', 'just', 'violence', 'but', 'injustice', 'crooked', 'guards', 'who', 'll', 'be', 'sold', 'out', 'for', 'a', 'nickel', 'inmates', 'who', 'll', 'kill', 'on', 'order', 'and', 'get', 'away', 'with', 'it', 'well', 'mannered', 'middle', 'class', 'inmates', 'being', 'turned', 'into', 'prison', 'bitches', 'due', 'to', 'their', 'lack', 'of', 'street', 'skills', 'or', 'prison', 'experience', 'watching', 'oz', 'you', 'may', 'become', 'comfortable', 'with', 'what', 'is', 'uncomfortable', 'viewing', 'thats', 'if', 'you', 'can', 'get', 'in', 'touch', 'with', 'your', 'darker', 'side'])]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = ft.RegexTokenizer(inputCol='review', outputCol='review_tok', pattern='\\s+|[,.\\\"/!]')\n",
    "tokenizer.transform(reviews).select('review_tok').take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "worthy-panel",
   "metadata": {},
   "source": [
    "Then remove stopwords in the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "experimental-blackberry",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(review_stop=['one', 'reviewers', 'mentioned', 'watching', '1', 'oz', 'episode', 'll', 'hooked', 'right', 'exactly', 'happened', 'first', 'thing', 'struck', 'oz', 'brutality', 'unflinching', 'scenes', 'violence', 'set', 'right', 'word', 'go', 'trust', 'show', 'faint', 'hearted', 'timid', 'show', 'pulls', 'punches', 'regards', 'drugs', 'sex', 'violence', 'hardcore', 'classic', 'use', 'word', 'called', 'oz', 'nickname', 'given', 'oswald', 'maximum', 'security', 'state', 'penitentary', 'focuses', 'mainly', 'emerald', 'city', 'experimental', 'section', 'prison', 'cells', 'glass', 'fronts', 'face', 'inwards', 'privacy', 'high', 'agenda', 'em', 'city', 'home', 'many', 'aryans', 'muslims', 'gangstas', 'latinos', 'christians', 'italians', 'irish', 'scuffles', 'death', 'stares', 'dodgy', 'dealings', 'shady', 'agreements', 'never', 'far', 'away', 'say', 'main', 'appeal', 'show', 'due', 'fact', 'goes', 'shows', 'wouldn', 'dare', 'forget', 'pretty', 'pictures', 'painted', 'mainstream', 'audiences', 'forget', 'charm', 'forget', 'romance', 'oz', 'doesn', 'mess', 'around', 'first', 'episode', 'ever', 'saw', 'struck', 'nasty', 'surreal', 'couldn', 'say', 'ready', 'watched', 'developed', 'taste', 'oz', 'got', 'accustomed', 'high', 'levels', 'graphic', 'violence', 'violence', 'injustice', 'crooked', 'guards', 'll', 'sold', 'nickel', 'inmates', 'll', 'kill', 'order', 'get', 'away', 'well', 'mannered', 'middle', 'class', 'inmates', 'turned', 'prison', 'bitches', 'due', 'lack', 'street', 'skills', 'prison', 'experience', 'watching', 'oz', 'may', 'become', 'comfortable', 'uncomfortable', 'viewing', 'thats', 'get', 'touch', 'darker', 'side'])]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords = ft.StopWordsRemover(inputCol=tokenizer.getOutputCol(), outputCol='review_stop')\n",
    "stopwords.transform(tokenizer.transform(reviews)).select('review_stop').take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "blind-context",
   "metadata": {},
   "source": [
    "Now same as what we did in the exercise, let's create `CountVectorizer` to transform the text into term frequency vector. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "injured-episode",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(review_tf=SparseVector(101111, {2: 1.0, 10: 1.0, 13: 2.0, 17: 2.0, 28: 1.0, 32: 1.0, 35: 3.0, 39: 1.0, 45: 2.0, 46: 1.0, 50: 1.0, 53: 1.0, 54: 2.0, 57: 1.0, 83: 1.0, 85: 1.0, 91: 1.0, 93: 1.0, 97: 1.0, 101: 2.0, 108: 1.0, 121: 1.0, 128: 3.0, 138: 2.0, 160: 1.0, 161: 1.0, 169: 1.0, 174: 1.0, 184: 1.0, 191: 2.0, 195: 1.0, 217: 1.0, 235: 1.0, 249: 1.0, 250: 1.0, 251: 1.0, 264: 1.0, 278: 1.0, 286: 2.0, 302: 1.0, 316: 1.0, 324: 1.0, 370: 1.0, 386: 1.0, 409: 2.0, 438: 1.0, 448: 4.0, 453: 1.0, 459: 1.0, 480: 1.0, 501: 1.0, 514: 1.0, 526: 1.0, 534: 2.0, 535: 1.0, 567: 2.0, 582: 1.0, 685: 1.0, 707: 3.0, 754: 1.0, 779: 1.0, 826: 1.0, 921: 1.0, 939: 1.0, 1073: 3.0, 1092: 1.0, 1108: 1.0, 1146: 1.0, 1181: 1.0, 1210: 1.0, 1292: 1.0, 1293: 1.0, 1313: 1.0, 1348: 1.0, 1462: 1.0, 1475: 1.0, 1496: 1.0, 1619: 1.0, 1897: 1.0, 1917: 1.0, 1936: 1.0, 2019: 1.0, 2115: 1.0, 2206: 1.0, 2328: 1.0, 2345: 1.0, 2375: 1.0, 2422: 1.0, 2472: 1.0, 2591: 1.0, 2790: 1.0, 2794: 1.0, 2863: 1.0, 2899: 1.0, 2971: 6.0, 3086: 1.0, 3135: 2.0, 3205: 1.0, 3705: 1.0, 3727: 1.0, 3987: 1.0, 4073: 1.0, 4571: 1.0, 4784: 1.0, 4907: 1.0, 4938: 1.0, 5274: 1.0, 5407: 1.0, 5832: 1.0, 6756: 1.0, 6808: 2.0, 6963: 1.0, 7082: 1.0, 7174: 1.0, 7506: 1.0, 7590: 1.0, 7739: 1.0, 7832: 1.0, 8087: 1.0, 8646: 1.0, 9090: 1.0, 10091: 1.0, 11404: 1.0, 11630: 1.0, 11947: 1.0, 12353: 1.0, 14712: 1.0, 14897: 1.0, 15151: 1.0, 16645: 1.0, 19313: 1.0, 22856: 1.0, 22860: 1.0, 25261: 1.0, 32925: 1.0, 40472: 1.0, 48779: 1.0, 50884: 1.0, 52966: 1.0}))]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf = ft.CountVectorizer(inputCol=stopwords.getOutputCol(), outputCol='review_tf')\n",
    "tokenized = stopwords.transform(tokenizer.transform(reviews))\n",
    "tf.fit(tokenized).transform(tokenized).select('review_tf').take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "olive-birmingham",
   "metadata": {},
   "source": [
    "Then we use the `LDA` model to do topic modeling. We create the model here with 30 topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "thick-butler",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.ml.clustering as clus\n",
    "lda = clus.LDA(k=30, optimizer='online', maxIter=10, featuresCol=tf.getOutputCol())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "insured-premiere",
   "metadata": {},
   "source": [
    "Now let's build the pipeline to train the topic model from the raw data. It will take a while to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "commercial-hurricane",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(topicDistribution=DenseVector([0.0002, 0.9945, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002])),\n",
       " Row(topicDistribution=DenseVector([0.0004, 0.9894, 0.0004, 0.0004, 0.0004, 0.0004, 0.0004, 0.0004, 0.0004, 0.0004, 0.0004, 0.0004, 0.0004, 0.0004, 0.0004, 0.0004, 0.0004, 0.0004, 0.0004, 0.0004, 0.0004, 0.0004, 0.0004, 0.0004, 0.0004, 0.0004, 0.0004, 0.0004, 0.0004, 0.0004])),\n",
       " Row(topicDistribution=DenseVector([0.0004, 0.9895, 0.0004, 0.0004, 0.0004, 0.0004, 0.0004, 0.0004, 0.0004, 0.0004, 0.0004, 0.0004, 0.0004, 0.0004, 0.0004, 0.0004, 0.0004, 0.0004, 0.0004, 0.0004, 0.0004, 0.0004, 0.0004, 0.0004, 0.0004, 0.0004, 0.0004, 0.0004, 0.0004, 0.0004])),\n",
       " Row(topicDistribution=DenseVector([0.0005, 0.9864, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005])),\n",
       " Row(topicDistribution=DenseVector([0.0003, 0.9927, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003]))]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "\n",
    "#[Your Code] to build a ML pipeline to fit LDA\n",
    "pipeline = Pipeline(stages=[tokenizer, stopwords, tf, lda])\n",
    "pipeline_model = pipeline.fit(reviews)\n",
    "\n",
    "topics = pipeline_model.transform(reviews)\n",
    "topics.select('topicDistribution').take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "assisted-cookbook",
   "metadata": {},
   "source": [
    "Let's see if we have properly discovered the topics. This is just the same code we display topics in the exercise - we will reuse it several times here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fabulous-senegal",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/spark/python/pyspark/sql/context.py:125: FutureWarning: Deprecated in 3.0.0. Use SparkSession.builder.getOrCreate() instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "topic: 0\n",
      "*************************\n",
      "film\n",
      "one\n",
      "movie\n",
      "like\n",
      "even\n",
      "well\n",
      "two\n",
      "story\n",
      "time\n",
      "life\n",
      "good\n",
      "people\n",
      "much\n",
      "never\n",
      "get\n",
      "characters\n",
      "character\n",
      "way\n",
      "see\n",
      "plot\n",
      "*************************\n",
      "topic: 1\n",
      "*************************\n",
      "movie\n",
      "film\n",
      "one\n",
      "like\n",
      "good\n",
      "even\n",
      "time\n",
      "really\n",
      "see\n",
      "story\n",
      "well\n",
      "bad\n",
      "get\n",
      "much\n",
      "first\n",
      "great\n",
      "people\n",
      "also\n",
      "made\n",
      "make\n",
      "*************************\n",
      "topic: 2\n",
      "*************************\n",
      "movie\n",
      "film\n",
      "like\n",
      "one\n",
      "good\n",
      "even\n",
      "really\n",
      "see\n",
      "films\n",
      "movies\n",
      "make\n",
      "first\n",
      "watch\n",
      "people\n",
      "bad\n",
      "seen\n",
      "time\n",
      "characters\n",
      "well\n",
      "made\n",
      "*************************\n",
      "topic: 3\n",
      "*************************\n",
      "film\n",
      "even\n",
      "like\n",
      "movie\n",
      "one\n",
      "pasolini\n",
      "horror\n",
      "rourke\n",
      "story\n",
      "show\n",
      "ben\n",
      "get\n",
      "really\n",
      "much\n",
      "actors\n",
      "time\n",
      "little\n",
      "rather\n",
      "mother\n",
      "make\n",
      "*************************\n",
      "topic: 4\n",
      "*************************\n",
      "bed\n",
      "bad\n",
      "film\n",
      "death\n",
      "lloyd\n",
      "action\n",
      "like\n",
      "see\n",
      "ve\n",
      "might\n",
      "horror\n",
      "movie\n",
      "dream\n",
      "shield\n",
      "best\n",
      "jackie\n",
      "performances\n",
      "one\n",
      "make\n",
      "chan\n",
      "*************************\n",
      "topic: 5\n",
      "*************************\n",
      "movie\n",
      "like\n",
      "good\n",
      "thrust\n",
      "even\n",
      "time\n",
      "really\n",
      "something\n",
      "get\n",
      "much\n",
      "people\n",
      "life\n",
      "film\n",
      "one\n",
      "every\n",
      "batman\n",
      "though\n",
      "see\n",
      "ever\n",
      "say\n",
      "*************************\n",
      "topic: 6\n",
      "*************************\n",
      "movie\n",
      "film\n",
      "one\n",
      "really\n",
      "first\n",
      "good\n",
      "story\n",
      "see\n",
      "like\n",
      "something\n",
      "much\n",
      "well\n",
      "think\n",
      "show\n",
      "thought\n",
      "great\n",
      "time\n",
      "never\n",
      "know\n",
      "didn\n",
      "*************************\n",
      "topic: 7\n",
      "*************************\n",
      "one\n",
      "movie\n",
      "film\n",
      "like\n",
      "good\n",
      "well\n",
      "character\n",
      "bad\n",
      "time\n",
      "great\n",
      "first\n",
      "even\n",
      "watch\n",
      "much\n",
      "characters\n",
      "people\n",
      "role\n",
      "best\n",
      "10\n",
      "make\n",
      "*************************\n",
      "topic: 8\n",
      "*************************\n",
      "film\n",
      "movie\n",
      "one\n",
      "like\n",
      "time\n",
      "see\n",
      "good\n",
      "story\n",
      "much\n",
      "really\n",
      "many\n",
      "watch\n",
      "films\n",
      "even\n",
      "best\n",
      "great\n",
      "way\n",
      "also\n",
      "life\n",
      "seen\n",
      "*************************\n",
      "topic: 9\n",
      "*************************\n",
      "movie\n",
      "film\n",
      "also\n",
      "great\n",
      "one\n",
      "like\n",
      "get\n",
      "made\n",
      "story\n",
      "good\n",
      "well\n",
      "best\n",
      "really\n",
      "watch\n",
      "cannibal\n",
      "role\n",
      "bad\n",
      "part\n",
      "actors\n",
      "big\n",
      "*************************\n",
      "topic: 10\n",
      "*************************\n",
      "movie\n",
      "one\n",
      "film\n",
      "like\n",
      "really\n",
      "story\n",
      "see\n",
      "scene\n",
      "well\n",
      "love\n",
      "many\n",
      "time\n",
      "first\n",
      "seen\n",
      "great\n",
      "best\n",
      "movies\n",
      "think\n",
      "seems\n",
      "made\n",
      "*************************\n",
      "topic: 11\n",
      "*************************\n",
      "movie\n",
      "one\n",
      "film\n",
      "time\n",
      "good\n",
      "great\n",
      "really\n",
      "well\n",
      "like\n",
      "bad\n",
      "watch\n",
      "made\n",
      "seen\n",
      "way\n",
      "much\n",
      "acting\n",
      "funny\n",
      "life\n",
      "ve\n",
      "old\n",
      "*************************\n",
      "topic: 12\n",
      "*************************\n",
      "film\n",
      "like\n",
      "one\n",
      "much\n",
      "well\n",
      "good\n",
      "films\n",
      "even\n",
      "see\n",
      "never\n",
      "also\n",
      "first\n",
      "really\n",
      "quite\n",
      "people\n",
      "time\n",
      "way\n",
      "story\n",
      "man\n",
      "character\n",
      "*************************\n",
      "topic: 13\n",
      "*************************\n",
      "film\n",
      "one\n",
      "movie\n",
      "like\n",
      "really\n",
      "well\n",
      "even\n",
      "story\n",
      "love\n",
      "scenes\n",
      "book\n",
      "made\n",
      "two\n",
      "life\n",
      "end\n",
      "make\n",
      "time\n",
      "many\n",
      "scene\n",
      "also\n",
      "*************************\n",
      "topic: 14\n",
      "*************************\n",
      "film\n",
      "movie\n",
      "one\n",
      "like\n",
      "good\n",
      "story\n",
      "see\n",
      "first\n",
      "even\n",
      "way\n",
      "great\n",
      "get\n",
      "films\n",
      "look\n",
      "time\n",
      "think\n",
      "love\n",
      "make\n",
      "series\n",
      "ever\n",
      "*************************\n",
      "topic: 15\n",
      "*************************\n",
      "film\n",
      "movie\n",
      "like\n",
      "one\n",
      "even\n",
      "good\n",
      "people\n",
      "really\n",
      "great\n",
      "acting\n",
      "well\n",
      "characters\n",
      "story\n",
      "character\n",
      "much\n",
      "time\n",
      "man\n",
      "movies\n",
      "see\n",
      "films\n",
      "*************************\n",
      "topic: 16\n",
      "*************************\n",
      "film\n",
      "one\n",
      "movie\n",
      "good\n",
      "like\n",
      "great\n",
      "story\n",
      "bad\n",
      "first\n",
      "time\n",
      "much\n",
      "show\n",
      "seen\n",
      "people\n",
      "think\n",
      "also\n",
      "really\n",
      "see\n",
      "well\n",
      "many\n",
      "*************************\n",
      "topic: 17\n",
      "*************************\n",
      "film\n",
      "one\n",
      "movie\n",
      "also\n",
      "best\n",
      "like\n",
      "good\n",
      "big\n",
      "going\n",
      "great\n",
      "puri\n",
      "director\n",
      "performance\n",
      "still\n",
      "even\n",
      "om\n",
      "way\n",
      "people\n",
      "watch\n",
      "g\n",
      "*************************\n",
      "topic: 18\n",
      "*************************\n",
      "movie\n",
      "one\n",
      "film\n",
      "like\n",
      "good\n",
      "see\n",
      "story\n",
      "get\n",
      "even\n",
      "people\n",
      "time\n",
      "much\n",
      "really\n",
      "also\n",
      "make\n",
      "well\n",
      "think\n",
      "plot\n",
      "show\n",
      "know\n",
      "*************************\n",
      "topic: 19\n",
      "*************************\n",
      "movie\n",
      "one\n",
      "film\n",
      "like\n",
      "good\n",
      "time\n",
      "story\n",
      "see\n",
      "life\n",
      "get\n",
      "movies\n",
      "well\n",
      "really\n",
      "many\n",
      "way\n",
      "long\n",
      "much\n",
      "made\n",
      "action\n",
      "little\n",
      "*************************\n",
      "topic: 20\n",
      "*************************\n",
      "movie\n",
      "one\n",
      "film\n",
      "even\n",
      "like\n",
      "get\n",
      "time\n",
      "really\n",
      "bad\n",
      "story\n",
      "well\n",
      "way\n",
      "something\n",
      "series\n",
      "show\n",
      "old\n",
      "films\n",
      "good\n",
      "plot\n",
      "two\n",
      "*************************\n",
      "topic: 21\n",
      "*************************\n",
      "one\n",
      "film\n",
      "like\n",
      "two\n",
      "movie\n",
      "first\n",
      "story\n",
      "way\n",
      "much\n",
      "well\n",
      "good\n",
      "people\n",
      "also\n",
      "character\n",
      "made\n",
      "see\n",
      "really\n",
      "even\n",
      "time\n",
      "characters\n",
      "*************************\n",
      "topic: 22\n",
      "*************************\n",
      "movie\n",
      "film\n",
      "one\n",
      "even\n",
      "like\n",
      "good\n",
      "get\n",
      "great\n",
      "story\n",
      "well\n",
      "much\n",
      "series\n",
      "best\n",
      "really\n",
      "bad\n",
      "many\n",
      "films\n",
      "movies\n",
      "first\n",
      "show\n",
      "*************************\n",
      "topic: 23\n",
      "*************************\n",
      "movie\n",
      "one\n",
      "film\n",
      "good\n",
      "like\n",
      "great\n",
      "really\n",
      "way\n",
      "time\n",
      "well\n",
      "bad\n",
      "get\n",
      "think\n",
      "story\n",
      "movies\n",
      "first\n",
      "much\n",
      "characters\n",
      "films\n",
      "two\n",
      "*************************\n",
      "topic: 24\n",
      "*************************\n",
      "film\n",
      "one\n",
      "movie\n",
      "story\n",
      "time\n",
      "like\n",
      "much\n",
      "two\n",
      "characters\n",
      "good\n",
      "see\n",
      "people\n",
      "show\n",
      "best\n",
      "way\n",
      "really\n",
      "even\n",
      "make\n",
      "character\n",
      "first\n",
      "*************************\n",
      "topic: 25\n",
      "*************************\n",
      "film\n",
      "one\n",
      "even\n",
      "movie\n",
      "time\n",
      "good\n",
      "well\n",
      "story\n",
      "much\n",
      "character\n",
      "part\n",
      "make\n",
      "really\n",
      "two\n",
      "know\n",
      "also\n",
      "role\n",
      "like\n",
      "films\n",
      "many\n",
      "*************************\n",
      "topic: 26\n",
      "*************************\n",
      "movie\n",
      "dillinger\n",
      "film\n",
      "like\n",
      "nelson\n",
      "snakes\n",
      "good\n",
      "baby\n",
      "first\n",
      "way\n",
      "one\n",
      "train\n",
      "shot\n",
      "get\n",
      "little\n",
      "also\n",
      "even\n",
      "thing\n",
      "face\n",
      "made\n",
      "*************************\n",
      "topic: 27\n",
      "*************************\n",
      "avro\n",
      "bood\n",
      "reclamation\n",
      "mescaleros\n",
      "ziga\n",
      "squeaked\n",
      "sevizia\n",
      "thunderstorm\n",
      "zalinsky\n",
      "finnish\n",
      "unresponsive\n",
      "distrustful\n",
      "conclusive\n",
      "sloe\n",
      "ladle\n",
      "rockit\n",
      "squish\n",
      "busom\n",
      "1845\n",
      "maddness\n",
      "*************************\n",
      "topic: 28\n",
      "*************************\n",
      "one\n",
      "movie\n",
      "game\n",
      "film\n",
      "really\n",
      "like\n",
      "fantasy\n",
      "good\n",
      "played\n",
      "two\n",
      "bad\n",
      "well\n",
      "people\n",
      "character\n",
      "club\n",
      "made\n",
      "never\n",
      "get\n",
      "better\n",
      "try\n",
      "*************************\n",
      "topic: 29\n",
      "*************************\n",
      "make\n",
      "story\n",
      "television\n",
      "century\n",
      "place\n",
      "bytes\n",
      "determine\n",
      "references\n",
      "production\n",
      "ritson\n",
      "away\n",
      "character\n",
      "bear\n",
      "displeasing\n",
      "1983\n",
      "series\n",
      "blake\n",
      "longer\n",
      "lips\n",
      "commercial\n",
      "*************************\n"
     ]
    }
   ],
   "source": [
    "#Code to extract topics from models\n",
    "vectorized_model = pipeline_model.stages[2]\n",
    "topic_model = pipeline_model.stages[3]\n",
    "vocab = vectorized_model.vocabulary\n",
    "topic_words_list = topic_model.describeTopics(20)\n",
    "topic_words_rdd = topic_words_list.rdd\n",
    "topics_words = topic_words_rdd.map(lambda row: row['termIndices']).map(lambda idx_list: [vocab[idx] for idx in idx_list]).collect()\n",
    "\n",
    "for idx, topic in enumerate(topics_words):\n",
    "    print(\"topic: {}\".format(idx))\n",
    "    print(\"*\"*25)\n",
    "    for word in topic:\n",
    "       print(word)\n",
    "    print(\"*\"*25)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "floating-dutch",
   "metadata": {},
   "source": [
    "How do you think about the topics? Do they make sense? If you think the topics we get from the movie reviews should be better, let's continue to see what we can do to make them better."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "copyrighted-burden",
   "metadata": {},
   "source": [
    "One possible reason is that we have many words that do not show up frequently. That is, they are very specific words to certain movies but don't occur across reviews. Such words are not very meaningful and they do not represent common themes in those reviews. So here we limit the frequency of words to at least 5 and run LDA with pipeline again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "devoted-spell",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(topicDistribution=DenseVector([0.9944, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002])),\n",
       " Row(topicDistribution=DenseVector([0.0005, 0.0004, 0.0004, 0.0004, 0.0004, 0.0004, 0.0004, 0.0004, 0.0004, 0.0004, 0.0004, 0.0004, 0.0004, 0.0004, 0.0004, 0.0004, 0.9891, 0.0004, 0.0004, 0.0004, 0.0004, 0.0004, 0.0004, 0.0004, 0.0004, 0.0004, 0.0004, 0.0004, 0.0004, 0.0004])),\n",
       " Row(topicDistribution=DenseVector([0.7046, 0.0004, 0.0004, 0.0004, 0.0004, 0.0004, 0.0004, 0.0004, 0.0004, 0.0004, 0.0004, 0.0004, 0.0004, 0.0004, 0.0004, 0.0004, 0.2852, 0.0004, 0.0004, 0.0004, 0.0004, 0.0004, 0.0004, 0.0004, 0.0004, 0.0004, 0.0004, 0.0004, 0.0004, 0.0004])),\n",
       " Row(topicDistribution=DenseVector([0.6288, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.3581, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005])),\n",
       " Row(topicDistribution=DenseVector([0.3155, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.6773, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003]))]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#filter the countvectorizer\n",
    "tf = ft.CountVectorizer(inputCol=stopwords.getOutputCol(), outputCol='review_tf', minDF=5)\n",
    "\n",
    "#[Your Code] to build a ML pipeline and fit LDA again\n",
    "pipeline = Pipeline(stages=[tokenizer, stopwords, tf, lda])\n",
    "pipeline_model = pipeline.fit(reviews)\n",
    "\n",
    "topics = pipeline_model.transform(reviews)\n",
    "topics.select('topicDistribution').take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "superb-fifth",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "topic: 0\n",
      "*************************\n",
      "movie\n",
      "film\n",
      "one\n",
      "like\n",
      "even\n",
      "good\n",
      "really\n",
      "bad\n",
      "time\n",
      "get\n",
      "see\n",
      "much\n",
      "well\n",
      "story\n",
      "people\n",
      "make\n",
      "first\n",
      "movies\n",
      "plot\n",
      "made\n",
      "*************************\n",
      "topic: 1\n",
      "*************************\n",
      "movie\n",
      "one\n",
      "good\n",
      "really\n",
      "great\n",
      "way\n",
      "like\n",
      "get\n",
      "see\n",
      "story\n",
      "well\n",
      "much\n",
      "film\n",
      "ghoulies\n",
      "also\n",
      "first\n",
      "think\n",
      "watch\n",
      "go\n",
      "best\n",
      "*************************\n",
      "topic: 2\n",
      "*************************\n",
      "film\n",
      "one\n",
      "time\n",
      "movie\n",
      "much\n",
      "like\n",
      "well\n",
      "story\n",
      "get\n",
      "films\n",
      "people\n",
      "life\n",
      "see\n",
      "acting\n",
      "good\n",
      "way\n",
      "really\n",
      "great\n",
      "pretty\n",
      "even\n",
      "*************************\n",
      "topic: 3\n",
      "*************************\n",
      "film\n",
      "movie\n",
      "one\n",
      "like\n",
      "good\n",
      "really\n",
      "story\n",
      "way\n",
      "even\n",
      "time\n",
      "much\n",
      "well\n",
      "bad\n",
      "character\n",
      "first\n",
      "get\n",
      "ve\n",
      "snakes\n",
      "didn\n",
      "custer\n",
      "*************************\n",
      "topic: 4\n",
      "*************************\n",
      "movie\n",
      "one\n",
      "like\n",
      "film\n",
      "something\n",
      "well\n",
      "even\n",
      "time\n",
      "good\n",
      "first\n",
      "people\n",
      "many\n",
      "see\n",
      "story\n",
      "get\n",
      "life\n",
      "way\n",
      "much\n",
      "films\n",
      "make\n",
      "*************************\n",
      "topic: 5\n",
      "*************************\n",
      "one\n",
      "film\n",
      "movie\n",
      "even\n",
      "much\n",
      "time\n",
      "really\n",
      "well\n",
      "good\n",
      "like\n",
      "story\n",
      "first\n",
      "films\n",
      "get\n",
      "characters\n",
      "two\n",
      "many\n",
      "scenes\n",
      "movies\n",
      "scene\n",
      "*************************\n",
      "topic: 6\n",
      "*************************\n",
      "one\n",
      "film\n",
      "movie\n",
      "show\n",
      "like\n",
      "much\n",
      "time\n",
      "well\n",
      "people\n",
      "also\n",
      "really\n",
      "even\n",
      "think\n",
      "two\n",
      "good\n",
      "life\n",
      "never\n",
      "documentary\n",
      "seen\n",
      "know\n",
      "*************************\n",
      "topic: 7\n",
      "*************************\n",
      "movie\n",
      "film\n",
      "like\n",
      "one\n",
      "even\n",
      "much\n",
      "good\n",
      "bad\n",
      "well\n",
      "story\n",
      "many\n",
      "thing\n",
      "also\n",
      "really\n",
      "see\n",
      "films\n",
      "characters\n",
      "re\n",
      "scenes\n",
      "better\n",
      "*************************\n",
      "topic: 8\n",
      "*************************\n",
      "film\n",
      "one\n",
      "like\n",
      "movie\n",
      "films\n",
      "good\n",
      "great\n",
      "first\n",
      "characters\n",
      "10\n",
      "movies\n",
      "make\n",
      "even\n",
      "see\n",
      "story\n",
      "think\n",
      "two\n",
      "bad\n",
      "lot\n",
      "people\n",
      "*************************\n",
      "topic: 9\n",
      "*************************\n",
      "film\n",
      "story\n",
      "man\n",
      "1\n",
      "see\n",
      "like\n",
      "even\n",
      "part\n",
      "better\n",
      "one\n",
      "well\n",
      "life\n",
      "good\n",
      "herr\n",
      "movie\n",
      "ever\n",
      "much\n",
      "2\n",
      "first\n",
      "car\n",
      "*************************\n",
      "topic: 10\n",
      "*************************\n",
      "film\n",
      "good\n",
      "one\n",
      "movie\n",
      "really\n",
      "bad\n",
      "first\n",
      "story\n",
      "time\n",
      "also\n",
      "like\n",
      "little\n",
      "get\n",
      "acting\n",
      "think\n",
      "see\n",
      "plot\n",
      "well\n",
      "movies\n",
      "far\n",
      "*************************\n",
      "topic: 11\n",
      "*************************\n",
      "movie\n",
      "one\n",
      "film\n",
      "like\n",
      "even\n",
      "leland\n",
      "good\n",
      "time\n",
      "two\n",
      "really\n",
      "first\n",
      "get\n",
      "make\n",
      "little\n",
      "much\n",
      "dillinger\n",
      "series\n",
      "well\n",
      "life\n",
      "best\n",
      "*************************\n",
      "topic: 12\n",
      "*************************\n",
      "movie\n",
      "film\n",
      "one\n",
      "like\n",
      "even\n",
      "people\n",
      "pretty\n",
      "get\n",
      "first\n",
      "see\n",
      "time\n",
      "really\n",
      "watching\n",
      "look\n",
      "something\n",
      "good\n",
      "make\n",
      "seen\n",
      "great\n",
      "say\n",
      "*************************\n",
      "topic: 13\n",
      "*************************\n",
      "film\n",
      "one\n",
      "really\n",
      "ae\n",
      "like\n",
      "shin\n",
      "best\n",
      "movie\n",
      "time\n",
      "well\n",
      "man\n",
      "life\n",
      "director\n",
      "also\n",
      "story\n",
      "scene\n",
      "much\n",
      "every\n",
      "mishima\n",
      "jeon\n",
      "*************************\n",
      "topic: 14\n",
      "*************************\n",
      "film\n",
      "movie\n",
      "one\n",
      "like\n",
      "time\n",
      "much\n",
      "way\n",
      "see\n",
      "people\n",
      "character\n",
      "really\n",
      "makes\n",
      "good\n",
      "seen\n",
      "well\n",
      "two\n",
      "watching\n",
      "story\n",
      "even\n",
      "know\n",
      "*************************\n",
      "topic: 15\n",
      "*************************\n",
      "movie\n",
      "film\n",
      "like\n",
      "one\n",
      "really\n",
      "story\n",
      "films\n",
      "good\n",
      "many\n",
      "also\n",
      "movies\n",
      "get\n",
      "series\n",
      "even\n",
      "well\n",
      "first\n",
      "seen\n",
      "see\n",
      "bad\n",
      "two\n",
      "*************************\n",
      "topic: 16\n",
      "*************************\n",
      "film\n",
      "movie\n",
      "one\n",
      "like\n",
      "great\n",
      "story\n",
      "good\n",
      "time\n",
      "see\n",
      "well\n",
      "also\n",
      "really\n",
      "love\n",
      "many\n",
      "first\n",
      "much\n",
      "life\n",
      "best\n",
      "people\n",
      "films\n",
      "*************************\n",
      "topic: 17\n",
      "*************************\n",
      "movie\n",
      "one\n",
      "film\n",
      "good\n",
      "first\n",
      "like\n",
      "never\n",
      "great\n",
      "see\n",
      "seen\n",
      "well\n",
      "time\n",
      "ever\n",
      "story\n",
      "man\n",
      "really\n",
      "way\n",
      "best\n",
      "also\n",
      "thought\n",
      "*************************\n",
      "topic: 18\n",
      "*************************\n",
      "movie\n",
      "film\n",
      "like\n",
      "really\n",
      "see\n",
      "good\n",
      "one\n",
      "bad\n",
      "story\n",
      "watch\n",
      "time\n",
      "even\n",
      "get\n",
      "make\n",
      "much\n",
      "movies\n",
      "great\n",
      "well\n",
      "man\n",
      "saw\n",
      "*************************\n",
      "topic: 19\n",
      "*************************\n",
      "film\n",
      "ustinov\n",
      "one\n",
      "like\n",
      "people\n",
      "show\n",
      "movie\n",
      "even\n",
      "reg\n",
      "see\n",
      "two\n",
      "newhart\n",
      "time\n",
      "train\n",
      "ever\n",
      "harry\n",
      "get\n",
      "kids\n",
      "ali\n",
      "good\n",
      "*************************\n",
      "topic: 20\n",
      "*************************\n",
      "movie\n",
      "bad\n",
      "film\n",
      "one\n",
      "characters\n",
      "story\n",
      "know\n",
      "love\n",
      "like\n",
      "people\n",
      "even\n",
      "way\n",
      "see\n",
      "scenes\n",
      "really\n",
      "good\n",
      "nothing\n",
      "acting\n",
      "think\n",
      "thing\n",
      "*************************\n",
      "topic: 21\n",
      "*************************\n",
      "film\n",
      "one\n",
      "movie\n",
      "like\n",
      "good\n",
      "great\n",
      "well\n",
      "even\n",
      "bad\n",
      "time\n",
      "never\n",
      "many\n",
      "films\n",
      "seen\n",
      "much\n",
      "way\n",
      "really\n",
      "best\n",
      "also\n",
      "ve\n",
      "*************************\n",
      "topic: 22\n",
      "*************************\n",
      "movie\n",
      "film\n",
      "one\n",
      "like\n",
      "even\n",
      "story\n",
      "bad\n",
      "good\n",
      "time\n",
      "see\n",
      "well\n",
      "great\n",
      "really\n",
      "man\n",
      "think\n",
      "films\n",
      "life\n",
      "much\n",
      "seen\n",
      "truffaut\n",
      "*************************\n",
      "topic: 23\n",
      "*************************\n",
      "movie\n",
      "film\n",
      "one\n",
      "like\n",
      "people\n",
      "good\n",
      "time\n",
      "see\n",
      "show\n",
      "well\n",
      "really\n",
      "much\n",
      "even\n",
      "plot\n",
      "way\n",
      "best\n",
      "made\n",
      "character\n",
      "many\n",
      "two\n",
      "*************************\n",
      "topic: 24\n",
      "*************************\n",
      "movie\n",
      "like\n",
      "film\n",
      "one\n",
      "good\n",
      "bad\n",
      "story\n",
      "movies\n",
      "even\n",
      "made\n",
      "people\n",
      "really\n",
      "make\n",
      "love\n",
      "watch\n",
      "acting\n",
      "time\n",
      "show\n",
      "first\n",
      "much\n",
      "*************************\n",
      "topic: 25\n",
      "*************************\n",
      "film\n",
      "nero\n",
      "bugs\n",
      "movie\n",
      "one\n",
      "dylan\n",
      "first\n",
      "vera\n",
      "well\n",
      "great\n",
      "even\n",
      "bunny\n",
      "really\n",
      "like\n",
      "way\n",
      "good\n",
      "lot\n",
      "time\n",
      "love\n",
      "also\n",
      "*************************\n",
      "topic: 26\n",
      "*************************\n",
      "movie\n",
      "one\n",
      "even\n",
      "good\n",
      "film\n",
      "character\n",
      "like\n",
      "batman\n",
      "get\n",
      "story\n",
      "see\n",
      "games\n",
      "actually\n",
      "well\n",
      "show\n",
      "people\n",
      "think\n",
      "way\n",
      "go\n",
      "haruhi\n",
      "*************************\n",
      "topic: 27\n",
      "*************************\n",
      "good\n",
      "great\n",
      "amazing\n",
      "action\n",
      "villain\n",
      "film\n",
      "lundgren\n",
      "dolph\n",
      "one\n",
      "plus\n",
      "character\n",
      "alex\n",
      "cool\n",
      "also\n",
      "opinion\n",
      "kids\n",
      "still\n",
      "corbucci\n",
      "lots\n",
      "favorite\n",
      "*************************\n",
      "topic: 28\n",
      "*************************\n",
      "film\n",
      "one\n",
      "movie\n",
      "like\n",
      "story\n",
      "much\n",
      "well\n",
      "films\n",
      "time\n",
      "people\n",
      "good\n",
      "first\n",
      "think\n",
      "characters\n",
      "m\n",
      "really\n",
      "see\n",
      "plot\n",
      "get\n",
      "made\n",
      "*************************\n",
      "topic: 29\n",
      "*************************\n",
      "movie\n",
      "sheba\n",
      "film\n",
      "one\n",
      "erika\n",
      "like\n",
      "pam\n",
      "really\n",
      "see\n",
      "get\n",
      "well\n",
      "grier\n",
      "good\n",
      "even\n",
      "many\n",
      "go\n",
      "ford\n",
      "things\n",
      "characters\n",
      "character\n",
      "*************************\n"
     ]
    }
   ],
   "source": [
    "#Code to extract topics from models\n",
    "vectorized_model = pipeline_model.stages[2]\n",
    "topic_model = pipeline_model.stages[3]\n",
    "vocab = vectorized_model.vocabulary\n",
    "topic_words_list = topic_model.describeTopics(20)\n",
    "topic_words_rdd = topic_words_list.rdd\n",
    "topics_words = topic_words_rdd.map(lambda row: row['termIndices']).map(lambda idx_list: [vocab[idx] for idx in idx_list]).collect()\n",
    "\n",
    "for idx, topic in enumerate(topics_words):\n",
    "    print(\"topic: {}\".format(idx))\n",
    "    print(\"*\"*25)\n",
    "    for word in topic:\n",
    "       print(word)\n",
    "    print(\"*\"*25)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "square-theorem",
   "metadata": {},
   "source": [
    "It is expected that the topics are getting better but still not very satisfying. Some words may be very specific to some reviews. Also, there are lots of words shown in different topics many times; possibly they are too common so they shouldn't be that important. Let's take one more step to use TF-IDF vector rather than TF vector. To build IF-IDF, we first create TF with CountVectorizer then create IDF from TF vector. Then we run LDA model with IF-IDF vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "hybrid-cause",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(topicDistribution=DenseVector([0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.2671, 0.2173, 0.0471, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.3312, 0.0001, 0.0812, 0.0001, 0.0001, 0.0547, 0.0001, 0.0001, 0.0001])),\n",
       " Row(topicDistribution=DenseVector([0.0001, 0.4362, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0895, 0.0001, 0.0001, 0.0001, 0.0472, 0.4238, 0.0001, 0.0001, 0.0001])),\n",
       " Row(topicDistribution=DenseVector([0.2869, 0.0001, 0.0001, 0.0001, 0.0436, 0.0001, 0.0536, 0.0001, 0.0001, 0.177, 0.0001, 0.0001, 0.0561, 0.0001, 0.0001, 0.0629, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.3173, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001])),\n",
       " Row(topicDistribution=DenseVector([0.2325, 0.103, 0.0001, 0.0001, 0.0001, 0.0623, 0.0001, 0.2726, 0.0002, 0.0002, 0.0001, 0.0001, 0.0618, 0.0002, 0.0002, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.1426, 0.1217, 0.0001, 0.0001, 0.0001, 0.0001, 0.0002, 0.0001, 0.0001, 0.0001])),\n",
       " Row(topicDistribution=DenseVector([0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0825, 0.0001, 0.1922, 0.082, 0.0001, 0.0001, 0.0001, 0.0001, 0.1224, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0888, 0.3363, 0.0001, 0.0001, 0.0001, 0.0935, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001]))]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#use tf-idf vector\n",
    "tf = ft.CountVectorizer(inputCol=stopwords.getOutputCol(), outputCol=\"review_tf\", vocabSize=10000)\n",
    "idf = ft.IDF(inputCol=tf.getOutputCol(), outputCol=\"review_tfidf\", minDocFreq=5)\n",
    "\n",
    "#[Your Code] to create a LDA model and put everything together into a ML pipeline to fit LDA\n",
    "lda = clus.LDA(k=30, optimizer='online', maxIter=10, featuresCol=idf.getOutputCol())\n",
    "pipeline = Pipeline(stages=[tokenizer, stopwords, tf, idf, lda])\n",
    "pipeline_model = pipeline.fit(reviews)\n",
    "\n",
    "topics = pipeline_model.transform(reviews)\n",
    "topics.select('topicDistribution').take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "surprising-cooper",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "topic: 0\n",
      "*************************\n",
      "movie\n",
      "film\n",
      "plot\n",
      "bad\n",
      "like\n",
      "funny\n",
      "even\n",
      "didn\n",
      "really\n",
      "good\n",
      "one\n",
      "much\n",
      "make\n",
      "time\n",
      "character\n",
      "nothing\n",
      "get\n",
      "think\n",
      "little\n",
      "know\n",
      "*************************\n",
      "topic: 1\n",
      "*************************\n",
      "chaplin\n",
      "film\n",
      "sandra\n",
      "columbo\n",
      "argento\n",
      "video\n",
      "movie\n",
      "alan\n",
      "bullock\n",
      "great\n",
      "good\n",
      "cinderella\n",
      "work\n",
      "bad\n",
      "many\n",
      "talented\n",
      "story\n",
      "like\n",
      "almighty\n",
      "10\n",
      "*************************\n",
      "topic: 2\n",
      "*************************\n",
      "freddy\n",
      "buddy\n",
      "chuck\n",
      "cage\n",
      "atlantis\n",
      "norris\n",
      "brothers\n",
      "nightmare\n",
      "widmark\n",
      "olivia\n",
      "race\n",
      "ring\n",
      "willie\n",
      "film\n",
      "milo\n",
      "destiny\n",
      "fuller\n",
      "movie\n",
      "shepherd\n",
      "sucked\n",
      "*************************\n",
      "topic: 3\n",
      "*************************\n",
      "che\n",
      "joan\n",
      "marie\n",
      "roy\n",
      "ryan\n",
      "dance\n",
      "rob\n",
      "fred\n",
      "jessica\n",
      "ben\n",
      "soderbergh\n",
      "astaire\n",
      "dan\n",
      "stone\n",
      "dancing\n",
      "paulie\n",
      "ginger\n",
      "rogers\n",
      "tim\n",
      "braveheart\n",
      "*************************\n",
      "topic: 4\n",
      "*************************\n",
      "match\n",
      "virus\n",
      "cole\n",
      "hindi\n",
      "malkovich\n",
      "kennedy\n",
      "willis\n",
      "fanny\n",
      "table\n",
      "segal\n",
      "driver\n",
      "ants\n",
      "bo\n",
      "khan\n",
      "championship\n",
      "loy\n",
      "homer\n",
      "kidman\n",
      "russo\n",
      "monkeys\n",
      "*************************\n",
      "topic: 5\n",
      "*************************\n",
      "tom\n",
      "hanks\n",
      "game\n",
      "boss\n",
      "paul\n",
      "kane\n",
      "football\n",
      "baseball\n",
      "film\n",
      "iron\n",
      "man\n",
      "sports\n",
      "carla\n",
      "mr\n",
      "george\n",
      "scorpion\n",
      "movie\n",
      "rockne\n",
      "rambo\n",
      "best\n",
      "*************************\n",
      "topic: 6\n",
      "*************************\n",
      "french\n",
      "le\n",
      "heist\n",
      "noir\n",
      "charlie\n",
      "rififi\n",
      "paris\n",
      "dassin\n",
      "grandfather\n",
      "andrews\n",
      "jonathan\n",
      "melville\n",
      "louis\n",
      "rouge\n",
      "jean\n",
      "whale\n",
      "dana\n",
      "snipes\n",
      "tony\n",
      "portuguese\n",
      "*************************\n",
      "topic: 7\n",
      "*************************\n",
      "hitchcock\n",
      "gypo\n",
      "jake\n",
      "oz\n",
      "natalie\n",
      "karen\n",
      "roosevelt\n",
      "cushing\n",
      "sitcom\n",
      "hardy\n",
      "arthur\n",
      "film\n",
      "laurel\n",
      "lee\n",
      "samurai\n",
      "lincoln\n",
      "house\n",
      "barry\n",
      "british\n",
      "uncle\n",
      "*************************\n",
      "topic: 8\n",
      "*************************\n",
      "film\n",
      "life\n",
      "women\n",
      "films\n",
      "men\n",
      "ford\n",
      "show\n",
      "german\n",
      "man\n",
      "family\n",
      "war\n",
      "many\n",
      "murder\n",
      "years\n",
      "gay\n",
      "one\n",
      "two\n",
      "young\n",
      "wife\n",
      "great\n",
      "*************************\n",
      "topic: 9\n",
      "*************************\n",
      "film\n",
      "disney\n",
      "series\n",
      "movie\n",
      "story\n",
      "episode\n",
      "time\n",
      "see\n",
      "like\n",
      "movies\n",
      "10\n",
      "much\n",
      "1\n",
      "really\n",
      "one\n",
      "bad\n",
      "way\n",
      "dvd\n",
      "lion\n",
      "world\n",
      "*************************\n",
      "topic: 10\n",
      "*************************\n",
      "blah\n",
      "kelly\n",
      "train\n",
      "snakes\n",
      "western\n",
      "larry\n",
      "scarecrow\n",
      "snake\n",
      "mummy\n",
      "film\n",
      "anne\n",
      "robot\n",
      "emperor\n",
      "gene\n",
      "mann\n",
      "aztec\n",
      "kids\n",
      "danny\n",
      "stupid\n",
      "films\n",
      "*************************\n",
      "topic: 11\n",
      "*************************\n",
      "caine\n",
      "helen\n",
      "eddie\n",
      "dahmer\n",
      "belushi\n",
      "car\n",
      "movie\n",
      "film\n",
      "keaton\n",
      "godzilla\n",
      "gere\n",
      "costello\n",
      "like\n",
      "diane\n",
      "josh\n",
      "captain\n",
      "guy\n",
      "racism\n",
      "white\n",
      "back\n",
      "*************************\n",
      "topic: 12\n",
      "*************************\n",
      "jack\n",
      "wayne\n",
      "film\n",
      "movie\n",
      "first\n",
      "get\n",
      "guilty\n",
      "gandhi\n",
      "one\n",
      "also\n",
      "cell\n",
      "films\n",
      "john\n",
      "lucas\n",
      "see\n",
      "half\n",
      "deneuve\n",
      "sex\n",
      "even\n",
      "good\n",
      "*************************\n",
      "topic: 13\n",
      "*************************\n",
      "horror\n",
      "bad\n",
      "movie\n",
      "film\n",
      "effects\n",
      "acting\n",
      "like\n",
      "even\n",
      "one\n",
      "movies\n",
      "get\n",
      "good\n",
      "gore\n",
      "really\n",
      "thing\n",
      "something\n",
      "seen\n",
      "make\n",
      "minutes\n",
      "time\n",
      "*************************\n",
      "topic: 14\n",
      "*************************\n",
      "show\n",
      "series\n",
      "film\n",
      "songs\n",
      "song\n",
      "funny\n",
      "dvd\n",
      "rock\n",
      "great\n",
      "christmas\n",
      "comedy\n",
      "characters\n",
      "band\n",
      "episodes\n",
      "tv\n",
      "loved\n",
      "animation\n",
      "humor\n",
      "well\n",
      "movie\n",
      "*************************\n",
      "topic: 15\n",
      "*************************\n",
      "victoria\n",
      "war\n",
      "queen\n",
      "marine\n",
      "film\n",
      "boxing\n",
      "military\n",
      "ray\n",
      "garfield\n",
      "charles\n",
      "sniper\n",
      "bridget\n",
      "life\n",
      "army\n",
      "young\n",
      "president\n",
      "two\n",
      "americans\n",
      "dolph\n",
      "foxx\n",
      "*************************\n",
      "topic: 16\n",
      "*************************\n",
      "book\n",
      "novel\n",
      "jackson\n",
      "russian\n",
      "scott\n",
      "film\n",
      "read\n",
      "movie\n",
      "ms\n",
      "robin\n",
      "desert\n",
      "story\n",
      "boll\n",
      "ricci\n",
      "ball\n",
      "williams\n",
      "lucille\n",
      "well\n",
      "man\n",
      "adaptation\n",
      "*************************\n",
      "topic: 17\n",
      "*************************\n",
      "batman\n",
      "harry\n",
      "eastwood\n",
      "clint\n",
      "series\n",
      "dirty\n",
      "penguin\n",
      "worms\n",
      "min\n",
      "fassbinder\n",
      "animated\n",
      "villains\n",
      "casper\n",
      "walt\n",
      "leonard\n",
      "chips\n",
      "joker\n",
      "faye\n",
      "oliver\n",
      "catwoman\n",
      "*************************\n",
      "topic: 18\n",
      "*************************\n",
      "bush\n",
      "ninja\n",
      "prot\n",
      "howard\n",
      "lou\n",
      "santa\n",
      "preston\n",
      "brooks\n",
      "arab\n",
      "bank\n",
      "david\n",
      "flash\n",
      "gangsters\n",
      "christie\n",
      "australia\n",
      "bob\n",
      "hitler\n",
      "sho\n",
      "satire\n",
      "latin\n",
      "*************************\n",
      "topic: 19\n",
      "*************************\n",
      "indian\n",
      "pacino\n",
      "streisand\n",
      "henry\n",
      "movie\n",
      "family\n",
      "brando\n",
      "myers\n",
      "ghosts\n",
      "macy\n",
      "ya\n",
      "michael\n",
      "barney\n",
      "show\n",
      "granger\n",
      "scooby\n",
      "see\n",
      "doo\n",
      "donald\n",
      "fay\n",
      "*************************\n",
      "topic: 20\n",
      "*************************\n",
      "zombie\n",
      "martial\n",
      "fu\n",
      "zombies\n",
      "arts\n",
      "kung\n",
      "film\n",
      "war\n",
      "action\n",
      "king\n",
      "wars\n",
      "world\n",
      "story\n",
      "fight\n",
      "movie\n",
      "first\n",
      "sky\n",
      "trilogy\n",
      "also\n",
      "miike\n",
      "*************************\n",
      "topic: 21\n",
      "*************************\n",
      "movie\n",
      "show\n",
      "really\n",
      "film\n",
      "think\n",
      "scene\n",
      "story\n",
      "re\n",
      "good\n",
      "watch\n",
      "like\n",
      "see\n",
      "well\n",
      "love\n",
      "say\n",
      "one\n",
      "characters\n",
      "first\n",
      "people\n",
      "watching\n",
      "*************************\n",
      "topic: 22\n",
      "*************************\n",
      "davis\n",
      "metal\n",
      "mst3k\n",
      "match\n",
      "bond\n",
      "vs\n",
      "moore\n",
      "mildred\n",
      "baker\n",
      "wwe\n",
      "bette\n",
      "wrestling\n",
      "vegas\n",
      "matches\n",
      "ken\n",
      "philip\n",
      "brosnan\n",
      "drill\n",
      "sunny\n",
      "poirot\n",
      "*************************\n",
      "topic: 23\n",
      "*************************\n",
      "jane\n",
      "emma\n",
      "novel\n",
      "austen\n",
      "cagney\n",
      "rochester\n",
      "thompson\n",
      "darwin\n",
      "hip\n",
      "gable\n",
      "hop\n",
      "version\n",
      "mayor\n",
      "stones\n",
      "dalton\n",
      "leno\n",
      "bogart\n",
      "paltrow\n",
      "eyre\n",
      "timothy\n",
      "*************************\n",
      "topic: 24\n",
      "*************************\n",
      "werewolf\n",
      "campbell\n",
      "altman\n",
      "bergman\n",
      "kurt\n",
      "victor\n",
      "davies\n",
      "hopper\n",
      "matthau\n",
      "montgomery\n",
      "nerd\n",
      "comedy\n",
      "buster\n",
      "simon\n",
      "glover\n",
      "akshay\n",
      "goldie\n",
      "reeves\n",
      "candy\n",
      "russell\n",
      "*************************\n",
      "topic: 25\n",
      "*************************\n",
      "tarzan\n",
      "jackie\n",
      "chan\n",
      "holmes\n",
      "series\n",
      "planet\n",
      "ann\n",
      "ship\n",
      "war\n",
      "custer\n",
      "episode\n",
      "space\n",
      "spock\n",
      "joe\n",
      "film\n",
      "trek\n",
      "navy\n",
      "kirk\n",
      "well\n",
      "star\n",
      "*************************\n",
      "topic: 26\n",
      "*************************\n",
      "japanese\n",
      "film\n",
      "movie\n",
      "people\n",
      "great\n",
      "story\n",
      "love\n",
      "music\n",
      "family\n",
      "movies\n",
      "also\n",
      "good\n",
      "made\n",
      "really\n",
      "actors\n",
      "one\n",
      "like\n",
      "see\n",
      "best\n",
      "way\n",
      "*************************\n",
      "topic: 27\n",
      "*************************\n",
      "stewart\n",
      "jimmy\n",
      "sellers\n",
      "freeman\n",
      "sheba\n",
      "terrorist\n",
      "show\n",
      "carlos\n",
      "grey\n",
      "grier\n",
      "kolchak\n",
      "timberlake\n",
      "kurtz\n",
      "australian\n",
      "morgan\n",
      "comedy\n",
      "baby\n",
      "streep\n",
      "grant\n",
      "pam\n",
      "*************************\n",
      "topic: 28\n",
      "*************************\n",
      "leland\n",
      "film\n",
      "human\n",
      "gamera\n",
      "christian\n",
      "bridge\n",
      "religion\n",
      "theory\n",
      "christ\n",
      "scott\n",
      "antonioni\n",
      "lives\n",
      "documentary\n",
      "movie\n",
      "true\n",
      "actions\n",
      "walker\n",
      "life\n",
      "even\n",
      "vs\n",
      "*************************\n",
      "topic: 29\n",
      "*************************\n",
      "seagal\n",
      "van\n",
      "lugosi\n",
      "mexican\n",
      "lynch\n",
      "page\n",
      "dr\n",
      "karloff\n",
      "bettie\n",
      "frankenstein\n",
      "scientist\n",
      "damme\n",
      "madonna\n",
      "mexico\n",
      "bela\n",
      "dracula\n",
      "monster\n",
      "rukh\n",
      "book\n",
      "derek\n",
      "*************************\n"
     ]
    }
   ],
   "source": [
    "#Code to extract topics from models\n",
    "tf_model = pipeline_model.stages[2]\n",
    "topic_model = pipeline_model.stages[4]\n",
    "vocab = tf_model.vocabulary\n",
    "topic_words_list = topic_model.describeTopics(20)\n",
    "topic_words_rdd = topic_words_list.rdd\n",
    "topics_words = topic_words_rdd.map(lambda row: row['termIndices']).map(lambda idx_list: [vocab[idx] for idx in idx_list]).collect()\n",
    "\n",
    "for idx, topic in enumerate(topics_words):\n",
    "    print(\"topic: {}\".format(idx))\n",
    "    print(\"*\"*25)\n",
    "    for word in topic:\n",
    "       print(word)\n",
    "    print(\"*\"*25)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "broadband-trader",
   "metadata": {},
   "source": [
    "The topics should be more reasonable now. You should believe they can still be further improved by cleaning up the text and tuning the hyperparameters but let's stop here for assignment. If you want to try yourself beyond the assignment, you can change the model configuration to see if you can get any further improvement."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "urban-shame",
   "metadata": {},
   "source": [
    "### Task 2 - Movie Review Sentiment Analysis with Spark ML\n",
    "The second task we are going to prediction. Let's continue with the reviews data and now we can do sentiment analysis with the TF-IDF. So with the TF-IDF vector, we can train and predict review sentiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "simple-princess",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|sentiment|\n",
      "+---------+\n",
      "| positive|\n",
      "| negative|\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#first let's confirm the potential labels\n",
    "#(it is possible sentiment can be neutral so we should make sure if that's the case)\n",
    "reviews.select('sentiment').distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "protecting-macintosh",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------+---------------+\n",
      "|              review|sentiment|sentiment_label|\n",
      "+--------------------+---------+---------------+\n",
      "|One of the other ...| positive|            1.0|\n",
      "|A wonderful littl...| positive|            1.0|\n",
      "|I thought this wa...| positive|            1.0|\n",
      "|Basically there s...| negative|            0.0|\n",
      "|Petter Mattei s  ...| positive|            1.0|\n",
      "|Probably my all t...| positive|            1.0|\n",
      "|I sure would like...| positive|            1.0|\n",
      "|This show was an ...| negative|            0.0|\n",
      "|Encouraged by the...| negative|            0.0|\n",
      "|If you like origi...| positive|            1.0|\n",
      "|Phil the Alien is...| negative|            0.0|\n",
      "|I saw this movie ...| negative|            0.0|\n",
      "|So im not a big f...| negative|            0.0|\n",
      "|The cast played S...| negative|            0.0|\n",
      "|This a fantastic ...| positive|            1.0|\n",
      "|Kind of drawn in ...| negative|            0.0|\n",
      "|Some films just s...| positive|            1.0|\n",
      "|This movie made i...| negative|            0.0|\n",
      "|I remember this f...| positive|            1.0|\n",
      "|An awful film  It...| negative|            0.0|\n",
      "+--------------------+---------+---------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#let's create the binary numerical lable from postive/negative\n",
    "reviews = reviews.withColumn('sentiment_label', fn.when(fn.col('sentiment')=='positive', 1.0).otherwise(0.0))\n",
    "reviews.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "becoming-soccer",
   "metadata": {},
   "outputs": [],
   "source": [
    "#split the training and testing set, with 80/20\n",
    "reviews_train, reviews_test = reviews.randomSplit([0.8, 0.2], seed=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "opposite-blackberry",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.ml.classification as cl\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "#craete logistic gression model and then build the pipeline to train the model\n",
    "lr = cl.LogisticRegression(maxIter=10, labelCol='sentiment_label', featuresCol=idf.getOutputCol())\n",
    "\n",
    "#[Your Code] to build a ML pipeline and train logistic regression model; then make predictions on testing data\n",
    "pipeline = Pipeline(stages=[tokenizer, stopwords, tf, idf, lr])\n",
    "lr_model = pipeline.fit(reviews_train)\n",
    "#make predictions with pipeline model\n",
    "predictions = lr_model.transform(reviews_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "coupled-allergy",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9338446403526336\n",
      "0.9273869863834877\n"
     ]
    }
   ],
   "source": [
    "import pyspark.ml.evaluation as ev\n",
    "#model evaluation for binary classification\n",
    "evaluator = ev.BinaryClassificationEvaluator(rawPredictionCol='probability', labelCol='sentiment_label')\n",
    "print(evaluator.evaluate(predictions, {evaluator.metricName: 'areaUnderROC'}))\n",
    "print(evaluator.evaluate(predictions, {evaluator.metricName: 'areaUnderPR'}))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "suitable-sphere",
   "metadata": {},
   "source": [
    "The prediction performance looks acceptable. Here note that TF-IDF is a long vector (here we select top 10000 words, but still a large number), so let's try something different. As mentioned in the class, another way to model text is word embedding with the Word2Vec model. So next we create word vector and use it to predict sentiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "technological-noise",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9332546784211575\n",
      "0.9305249621930078\n"
     ]
    }
   ],
   "source": [
    "#create word2vec model\n",
    "word2vec = ft.Word2Vec(vectorSize=100, minCount=5, inputCol=stopwords.getOutputCol(), outputCol=\"review_word2vec\")\n",
    "\n",
    "#same logistic regression model, but take output from word2vec model\n",
    "#[Your Code] to create a logistic regression model and build pipeline with word2vec to train logistic regession; then make predictions and evaluate model (areaUnderROC and areaUnderPR)\n",
    "lr = cl.LogisticRegression(maxIter=10, labelCol='sentiment_label', featuresCol=word2vec.getOutputCol())\n",
    "#ml pipeline\n",
    "pipeline = Pipeline(stages=[tokenizer, stopwords, word2vec, lr])\n",
    "lr_model = pipeline.fit(reviews_train)\n",
    "\n",
    "predictions = lr_model.transform(reviews_test)\n",
    "\n",
    "#model evaluation\n",
    "print(evaluator.evaluate(predictions, {evaluator.metricName: 'areaUnderROC'}))\n",
    "print(evaluator.evaluate(predictions, {evaluator.metricName: 'areaUnderPR'}))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "accepted-national",
   "metadata": {},
   "source": [
    "Here we can see the prediction is still acceptable, even though the number of features is only 100 now; word2vec model is a very useful representation of words and it reduces dimensionality significantly. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "improved-blowing",
   "metadata": {},
   "source": [
    "With this model, let's check if the number of features in word2vec model (i.e., vector size) matters in prediction. So in the exercise, we used CrossValidator to do hyperparameter tuning and choose the best model. Here for simplicity, we just do the same thing to compare two models with the vector size of word2vec as 100 and 200."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "controversial-contrary",
   "metadata": {},
   "source": [
    "Note this part will take a long time to run as we train multiple models together and building word2vec model will take a long time. This part is optinal so you may complete other parts first then try it yourself. Be patient and you can check the terminal where you start the container to confirm if it is still running."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "precious-heart",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9368438589461023\n",
      "0.9336204283302868\n"
     ]
    }
   ],
   "source": [
    "import pyspark.ml.tuning as tune\n",
    "\n",
    "paramGrid = tune.ParamGridBuilder().addGrid(word2vec.vectorSize, [100, 200]).build()\n",
    "crossval = tune.CrossValidator(estimator=pipeline, estimatorParamMaps=paramGrid, evaluator=evaluator)\n",
    "#the code below can do 5-folds cross validation at the same time, but it takes too long so commented\n",
    "#crossval = tune.CrossValidator(estimator=pipeline, estimatorParamMaps=paramGrid, evaluator=evaluator, numFolds=5)\n",
    "\n",
    "crossval_model = crossval.fit(reviews_train)\n",
    "predictions = crossval_model.transform(reviews_test)\n",
    "\n",
    "print(evaluator.evaluate(predictions, {evaluator.metricName: 'areaUnderROC'}))\n",
    "print(evaluator.evaluate(predictions, {evaluator.metricName: 'areaUnderPR'}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "round-queen",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[([{'vectorSize': 200}], 0.9274808569345858),\n",
       " ([{'vectorSize': 100}], 0.925447839715015)]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#this code will get the metrics for each model in CrossValidator\n",
    "results = []\n",
    "for params, metric in zip(crossval_model.getEstimatorParamMaps(), crossval_model.avgMetrics):\n",
    "    parameters = []\n",
    "    for key, paramValue in zip(params.keys(), params.values()):\n",
    "        parameters.append({key.name: paramValue})\n",
    "    results.append((parameters, metric))\n",
    "\n",
    "sorted(results, key=lambda x: x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "twelve-parking",
   "metadata": {},
   "source": [
    "You can see with more features in word2vec, the performance increases but it is very limited. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hearing-terrain",
   "metadata": {},
   "source": [
    "In the end, let's try an alternative model. In classfication, Support Vector Machine (SVM) is commonly used and let's see how we can use it here. We will keep the orginal configuration of word2vec model (vector size is 100) here for SVM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "correct-order",
   "metadata": {},
   "outputs": [],
   "source": [
    "#same word2vec model configuration is adopted here\n",
    "word2vec = ft.Word2Vec(vectorSize=100, minCount=5, inputCol=stopwords.getOutputCol(), outputCol=\"review_word2vec\")\n",
    "#create svm with LinearSVC, with features from word2vec model outputCol\n",
    "svm = cl.LinearSVC(maxIter=10, labelCol='sentiment_label', featuresCol=word2vec.getOutputCol())\n",
    "\n",
    "#build the ml pipeline and train the model; then make predictions\n",
    "#[Your Code] to build the ML pipeline to train SVM model; then make predictions (no need to evaluate, the evaluation is slightly different here so provided below)\n",
    "pipeline = Pipeline(stages=[tokenizer, stopwords, word2vec, svm])\n",
    "svm_model = pipeline.fit(reviews_train)\n",
    "\n",
    "predictions = svm_model.transform(reviews_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "whole-parts",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9322798637547595\n",
      "0.9300323457252992\n"
     ]
    }
   ],
   "source": [
    "#model evaluation, here slightly different for LinearSVC\n",
    "evaluator = ev.BinaryClassificationEvaluator(rawPredictionCol='rawPrediction', labelCol='sentiment_label')\n",
    "print(evaluator.evaluate(predictions, {evaluator.metricName: 'areaUnderROC'}))\n",
    "print(evaluator.evaluate(predictions, {evaluator.metricName: 'areaUnderPR'}))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nervous-proof",
   "metadata": {},
   "source": [
    "### Task 3 - Combine Spark ML and Streaming Analysis\n",
    "Now we have our sentiment prediction model with acceptable predictive performance. The last task is to combine this machine learning mode with spark streaming. That is, with a data stream, we will use the trained model to make real-time predictions. We will still use IMDB reviews and here we will create a simulated data stream from files in a folder, then receive the review stream and predict sentiment using spark structured streaming."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "plastic-dining",
   "metadata": {},
   "source": [
    "We will first create multiple files so that we can simulate a data stream to read files incrementally from a folder. To do that, we use the code below - it creates a folder 'review_stream', turn the spark dataframe into pandas dataframe (this is not a recommended approach but here we use it for convenience and conciseness), split the data into 100 smaller ones, and then save each smaller csv file into the folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "blank-officer",
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_test_stream = reviews_test.withColumn('review_id', fn.monotonically_increasing_id())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "interracial-reception",
   "metadata": {},
   "outputs": [],
   "source": [
    "#if you use cluster, do not run the code below, just stream from s3 directory with code for cluster in the next cell \n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "#code for local mode\n",
    "#here we just create a local directory to simulate the data stream\n",
    "if not os.path.exists('review_stream'):\n",
    "    os.mkdir('review_stream')\n",
    "\n",
    "#here we use pandas at local docker to speed up, with spark it is slow\n",
    "df = reviews_test_stream.orderBy('review_id').toPandas()\n",
    "#here we write a small number of rows into each csv file and read data stream from these files\n",
    "#we can split the dataframe into 100 smaller ones so each one has 100 rows\n",
    "for i,chunk in enumerate(np.array_split(df, 100)):\n",
    "    chunk.to_csv('./review_stream/reviews_{}.csv'.format(i), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "enclosed-assessment",
   "metadata": {},
   "source": [
    "Now we can read from the stream. For convenience, we just take the existing spark dataframe to reuse the schema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "assured-symbol",
   "metadata": {},
   "outputs": [],
   "source": [
    "#read from data stream in a folder\n",
    "streaming_review = spark.readStream.schema(reviews_test_stream.schema).option(\"maxFilesPerTrigger\", 1).csv('./review_stream')\n",
    "#for cluster\n",
    "#streaming_review = spark.readStream.schema(reviews_test_stream.schema).option(\"maxFilesPerTrigger\", 1).csv('s3://msbx5420-spr22/review_stream')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "neutral-victoria",
   "metadata": {},
   "source": [
    "Here from the data stream, we want to know two results. First, how many positive or negative reviews we have received in real time? Second, how many positive or negative reviews in each time window (so we know whether there is a peak of positive or negative review in certain time)? So we will do some calculations, and to capture time window, we will use the current timestamp to create 'processing_time' (the time we receive the data) and apply window on this timestamp."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "binary-kinase",
   "metadata": {},
   "outputs": [],
   "source": [
    "streaming_review_time = streaming_review.withColumn('processing_time', fn.current_timestamp())\n",
    "\n",
    "#we can just take the pipeline model we have trained to make prediction\n",
    "#[Your Code] to use the last pipeline model with SVM to make predictions on 'streaming_review_time'; save the result as 'streaming_sentiment'\n",
    "streaming_sentiment = svm_model.transform(streaming_review_time)\n",
    "\n",
    "#create a predicted text label from 'predicted column' is the prediction\n",
    "streaming_sentiment = streaming_sentiment.withColumn('predicted', fn.when(fn.col('prediction')==1.0, 'positive').otherwise('negative'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "stainless-institution",
   "metadata": {},
   "outputs": [],
   "source": [
    "#here we do transformations to get the results we need\n",
    "#first, get total number of positive and negative reviews we have received\n",
    "streaming_sentiment_count = streaming_sentiment.groupBy('predicted').count()\n",
    "\n",
    "#second, still number of positive and negative reviews we received, but by time window (60 seconds)\n",
    "streaming_sentiment_window_count = streaming_sentiment.groupBy(fn.window('processing_time', '60 seconds'), 'predicted').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "marked-religion",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "#now we have two streaming dataframe results\n",
    "print(streaming_sentiment_count.isStreaming)\n",
    "print(streaming_sentiment_window_count.isStreaming)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "aging-proxy",
   "metadata": {},
   "outputs": [],
   "source": [
    "#now we can define query to start streaming analysis and set the result table as 'sentiment'\n",
    "query_sentiment = (streaming_sentiment_count.writeStream.format(\"memory\").queryName(\"sentiment\").outputMode(\"complete\").start())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "stupid-middle",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define another query for the result table of windowed positive and negative review counts\n",
    "#[Your Code] to define the second query, name of result table is 'sentiment_window'\n",
    "query_sentiment_window = (streaming_sentiment_window_count.writeStream.format(\"memory\").queryName(\"sentiment_window\").outputMode(\"complete\").start())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "disciplinary-treasury",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----+\n",
      "|predicted|count|\n",
      "+---------+-----+\n",
      "| positive| 2135|\n",
      "| negative| 2006|\n",
      "+---------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#query the first result table to monitor real time results\n",
    "spark.sql('select * from sentiment').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "distinguished-activation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------------+---------+-----+\n",
      "|window                                    |predicted|count|\n",
      "+------------------------------------------+---------+-----+\n",
      "|{2022-03-07 06:47:00, 2022-03-07 06:48:00}|positive |970  |\n",
      "|{2022-03-07 06:47:00, 2022-03-07 06:48:00}|negative |848  |\n",
      "|{2022-03-07 06:48:00, 2022-03-07 06:49:00}|negative |1015 |\n",
      "|{2022-03-07 06:48:00, 2022-03-07 06:49:00}|positive |1005 |\n",
      "|{2022-03-07 06:49:00, 2022-03-07 06:50:00}|negative |46   |\n",
      "|{2022-03-07 06:49:00, 2022-03-07 06:50:00}|positive |55   |\n",
      "+------------------------------------------+---------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#query the second result table to monitor real time results, order the results by window\n",
    "spark.sql('select * from sentiment_window order by window').show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "parallel-daisy",
   "metadata": {},
   "outputs": [],
   "source": [
    "#stop query to finish streaming analysis\n",
    "query_sentiment.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "laughing-extraction",
   "metadata": {},
   "outputs": [],
   "source": [
    "#stop query to finish streaming analysis\n",
    "query_sentiment_window.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
