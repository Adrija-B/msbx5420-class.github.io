{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cardiac-preservation",
   "metadata": {},
   "source": [
    "## MSBX 5420 Assignment 3\n",
    "This assignment is about Spark Machine Learning and Spark Streaming. First two tasks focus on machine learning, and the third one combines machine learning and streaming analysis. We will use IMDB reviews data for the whole assignment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "recreational-agenda",
   "metadata": {},
   "source": [
    "### Task 1 - Topic Modeling on Moive Reviews with Spark ML\n",
    "First of all, let's load the data. The data structure is very simple.One column is review text, and another column is the label of review sentiment (positive or negative). Same as exercise, we can load .csv.gz file directly from spark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "thirty-yemen",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.master('local[4]').config(\"spark.executor.memory\", \"1g\").config(\"spark.driver.memory\", \"2g\").appName('spark_ml_imdb').getOrCreate()\n",
    "#for cluster - change kernel to PySpark\n",
    "#spark = SparkSession.builder.master('spark://spark-master:7077').appName('spark_ml_imdb').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "joint-marble",
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews = spark.read.options(inferSchema = True, multiLine = True, escape = '\\\"').csv('IMDB_Reviews.csv.gz', header=True)\n",
    "#for cluster\n",
    "#reviews = spark.read.options(inferSchema = True, multiLine = True, escape = '\\\"').csv('s3://msbx5420-spr21/zhiyiwang/IMDB_Reviews.csv.gz', header=True)\n",
    "reviews.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "incoming-fruit",
   "metadata": {},
   "source": [
    "First, we should clean up the review texts. Besides those special characters we have tried to remove in exercise, here we also need to remove the html tags in the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "loaded-print",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as fn\n",
    "import pyspark.ml.feature as ft\n",
    "\n",
    "#remove html tags in the text with regular expression\n",
    "reviews = reviews.withColumn('review', fn.regexp_replace(fn.col(\"review\"), '<[^>]+>', ' '))\n",
    "#remove special characters and line breaks in the text with regular expression\n",
    "reviews = reviews.withColumn('review', fn.regexp_replace(fn.col(\"review\"), '([^\\s\\w_]|_)+', ' ')).withColumn('review', fn.regexp_replace(fn.col(\"review\"), '[\\n\\r]', ' '))\n",
    "reviews.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "increasing-layout",
   "metadata": {},
   "source": [
    "Now let's create tokenizer to start the data processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "improving-tiffany",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = ft.RegexTokenizer(inputCol='review', outputCol='review_tok', pattern='\\s+|[,.\\\"/!]')\n",
    "tokenizer.transform(reviews).select('review_tok').take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "endangered-marks",
   "metadata": {},
   "source": [
    "Then remove stopwords in the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stretch-browse",
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = ft.StopWordsRemover(inputCol=tokenizer.getOutputCol(), outputCol='review_stop')\n",
    "stopwords.transform(tokenizer.transform(reviews)).select('review_stop').take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "optical-knight",
   "metadata": {},
   "source": [
    "Now same as what we did in the exercise, let's create `CountVectorizer` to transform the text into term frequency vector. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "interstate-organic",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "tf = ft.CountVectorizer(inputCol=stopwords.getOutputCol(), outputCol='review_tf')\n",
    "tokenized = stopwords.transform(tokenizer.transform(reviews))\n",
    "tf.fit(tokenized).transform(tokenized).select('review_tf').take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dominican-sugar",
   "metadata": {},
   "source": [
    "Then we use the `LDA` model to do topic modeling. We create the model here with 30 topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "parallel-heater",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.ml.clustering as clus\n",
    "lda = clus.LDA(k=30, optimizer='online', maxIter=10, featuresCol=tf.getOutputCol())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "warming-maryland",
   "metadata": {},
   "source": [
    "Now let's build the pipeline to train the topic model from the raw data. It will take a while to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "indoor-custody",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "\n",
    "#[Your Code] to build a ML pipeline to fit LDA\n",
    "\n",
    "\n",
    "#topics = pipeline_model.transform(reviews)\n",
    "#topics.select('topicDistribution').take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "optical-equality",
   "metadata": {},
   "source": [
    "Let's see if we have properly discovered the topics. This is just the same code we display topics in the exercise - we will reuse it several times here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "chronic-phrase",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Code to extract topics from models\n",
    "vectorized_model = pipeline_model.stages[2]\n",
    "topic_model = pipeline_model.stages[3]\n",
    "vocab = vectorized_model.vocabulary\n",
    "topic_words_list = topic_model.describeTopics(20)\n",
    "topic_words_rdd = topic_words_list.rdd\n",
    "topics_words = topic_words_rdd.map(lambda row: row['termIndices']).map(lambda idx_list: [vocab[idx] for idx in idx_list]).collect()\n",
    "\n",
    "for idx, topic in enumerate(topics_words):\n",
    "    print(\"topic: {}\".format(idx))\n",
    "    print(\"*\"*25)\n",
    "    for word in topic:\n",
    "       print(word)\n",
    "    print(\"*\"*25)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "prospective-campus",
   "metadata": {},
   "source": [
    "How do you think about the topics? Do they make sense? If you think the topics we get from the movie reviews should be better, let's continue to see what we can do to make them better."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "juvenile-shelter",
   "metadata": {},
   "source": [
    "One possible reason is that we have many words that do not show up frequently. That is, they are very specific words to certain movies but don't occur across reviews. Such words are not very meaningful and they do not represent common themes in those reviews. So here we limit the frequency of words to at least 5 and run LDA with pipeline again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unable-radiation",
   "metadata": {},
   "outputs": [],
   "source": [
    "#filter the countvectorizer\n",
    "tf = ft.CountVectorizer(inputCol=stopwords.getOutputCol(), outputCol='review_tf', minDF=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "positive-homework",
   "metadata": {},
   "outputs": [],
   "source": [
    "#[Your Code] to build a ML pipeline and fit LDA again\n",
    "\n",
    "\n",
    "#topics = pipeline_model.transform(reviews)\n",
    "#topics.select('topicDistribution').take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "incredible-bumper",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Code to extract topics from models\n",
    "vectorized_model = pipeline_model.stages[2]\n",
    "topic_model = pipeline_model.stages[3]\n",
    "vocab = vectorized_model.vocabulary\n",
    "topic_words_list = topic_model.describeTopics(20)\n",
    "topic_words_rdd = topic_words_list.rdd\n",
    "topics_words = topic_words_rdd.map(lambda row: row['termIndices']).map(lambda idx_list: [vocab[idx] for idx in idx_list]).collect()\n",
    "\n",
    "for idx, topic in enumerate(topics_words):\n",
    "    print(\"topic: {}\".format(idx))\n",
    "    print(\"*\"*25)\n",
    "    for word in topic:\n",
    "       print(word)\n",
    "    print(\"*\"*25)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "medium-locking",
   "metadata": {},
   "source": [
    "It is expected that the topics are getting better but still not very satisfying. There are lots of words shown in different topics many times; possibly they are too common so they shouldn't be that important. Let's take one more step to use TF-IDF vector rather than TF vector. To build IF-IDF, we first create TF with CountVectorizer then create IDF from TF vector. Then we run LDA model with IF-IDF vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "average-nothing",
   "metadata": {},
   "outputs": [],
   "source": [
    "#use tf-idf vector\n",
    "tf = ft.CountVectorizer(inputCol=stopwords.getOutputCol(), outputCol=\"review_tf\", vocabSize=10000)\n",
    "idf = ft.IDF(inputCol=tf.getOutputCol(), outputCol=\"review_tfidf\", minDocFreq=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "primary-principal",
   "metadata": {},
   "outputs": [],
   "source": [
    "#[Your Code] to create a LDA model (30 topics) and put everything together into a ML pipeline to fit LDA\n",
    "\n",
    "\n",
    "#topics = pipeline_model.transform(reviews)\n",
    "#topics.select('topicDistribution').take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "miniature-anniversary",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Code to extract topics from models\n",
    "tf_model = pipeline_model.stages[2]\n",
    "topic_model = pipeline_model.stages[4]\n",
    "vocab = tf_model.vocabulary\n",
    "topic_words_list = topic_model.describeTopics(20)\n",
    "topic_words_rdd = topic_words_list.rdd\n",
    "topics_words = topic_words_rdd.map(lambda row: row['termIndices']).map(lambda idx_list: [vocab[idx] for idx in idx_list]).collect()\n",
    "\n",
    "for idx, topic in enumerate(topics_words):\n",
    "    print(\"topic: {}\".format(idx))\n",
    "    print(\"*\"*25)\n",
    "    for word in topic:\n",
    "       print(word)\n",
    "    print(\"*\"*25)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "working-intersection",
   "metadata": {},
   "source": [
    "The topics should be more reasonable now. You should believe they can still be further improved by cleaning up the text and tuning the hyperparameters but let's stop here for assignment. If you want to try yourself beyond the assignment, you can change the model configuration to see if you can get any further improvement."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "controversial-bennett",
   "metadata": {},
   "source": [
    "### Task 2 - Movie Review Sentiment Analysis with Spark ML\n",
    "The second task we are going to prediction. Let's continue with the reviews data and now we can do sentiment analysis with the TF-IDF. So with the TF-IDF vector, we can train and predict review sentiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accompanied-congress",
   "metadata": {},
   "outputs": [],
   "source": [
    "#first let's confirm the potential labels\n",
    "#(it is possible sentiment can be neutral so we should make sure if that's the case)\n",
    "reviews.select('sentiment').distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "guided-august",
   "metadata": {},
   "outputs": [],
   "source": [
    "#let's create the binary numerical lable from postive/negative\n",
    "reviews = reviews.withColumn('sentiment_label', fn.when(fn.col('sentiment')=='positive', 1.0).otherwise(0.0))\n",
    "reviews.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dying-concentration",
   "metadata": {},
   "outputs": [],
   "source": [
    "#split the training and testing set, with 80/20\n",
    "reviews_train, reviews_test = reviews.randomSplit([0.8, 0.2], seed=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "assured-pattern",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.ml.classification as cl\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "#craete logistic gression model and then build the pipeline to train the model\n",
    "lr = cl.LogisticRegression(maxIter=10, labelCol='sentiment_label', featuresCol=idf.getOutputCol())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sized-george",
   "metadata": {},
   "outputs": [],
   "source": [
    "#[Your Code] to build a ML pipeline and train logistic regression model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "portuguese-manhattan",
   "metadata": {},
   "outputs": [],
   "source": [
    "#make predictions with pipeline model\n",
    "predictions = lr_model.transform(reviews_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "simple-drove",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.ml.evaluation as ev\n",
    "#model evaluation for binary classification\n",
    "evaluator = ev.BinaryClassificationEvaluator(rawPredictionCol='probability', labelCol='sentiment_label')\n",
    "print(evaluator.evaluate(predictions, {evaluator.metricName: 'areaUnderROC'}))\n",
    "print(evaluator.evaluate(predictions, {evaluator.metricName: 'areaUnderPR'}))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "seven-extraction",
   "metadata": {},
   "source": [
    "The prediction performance looks acceptable. Here note that TF-IDF is a long vector (here we select top 10000 words, but still a large number), so let's try something different. As mentioned in the class, another way to model text is word embedding with the Word2Vec model. So next we create word vector and use it to predict sentiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "protective-syndrome",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create word2vec model\n",
    "word2vec = ft.Word2Vec(vectorSize=100, minCount=5, inputCol=stopwords.getOutputCol(), outputCol=\"review_word2vec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "coordinated-temperature",
   "metadata": {},
   "outputs": [],
   "source": [
    "#same logistic regression model, but take output from word2vec model\n",
    "#[Your Code] to create a logistic regression model and build pipeline with word2vec to train logistic regession; then make predictions and evaluate model (areaUnderROC and areaUnderPR)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alpha-windsor",
   "metadata": {},
   "source": [
    "Check the prediction performance with only 100 features; word2vec model is a very useful representation of words and it reduces dimensionality significantly. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "recognized-donor",
   "metadata": {},
   "source": [
    "In the end, let's try an alternative model. In classfication, Support Vector Machine (SVM) is commonly used and let's see how we can use it here. We will keep the orginal configuration of word2vec model (vector size is 100) here for SVM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "portuguese-netscape",
   "metadata": {},
   "outputs": [],
   "source": [
    "#same word2vec model configuration is adopted here\n",
    "word2vec = ft.Word2Vec(vectorSize=100, minCount=5, inputCol=stopwords.getOutputCol(), outputCol=\"review_word2vec\")\n",
    "#create svm with LinearSVC, with features from word2vec model outputCol\n",
    "svm = cl.LinearSVC(maxIter=10, labelCol='sentiment_label', featuresCol=word2vec.getOutputCol())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "heated-spread",
   "metadata": {},
   "outputs": [],
   "source": [
    "#build the ml pipeline and train the model; then make predictions \n",
    "#[Your Code] to build the ML pipeline to train SVM model; then make predictions (no need to evaluate, the evaluation is slightly different here so provided below)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "several-burke",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model evaluation, here slightly different for LinearSVC\n",
    "evaluator = ev.BinaryClassificationEvaluator(rawPredictionCol='rawPrediction', labelCol='sentiment_label')\n",
    "print(evaluator.evaluate(predictions, {evaluator.metricName: 'areaUnderROC'}))\n",
    "print(evaluator.evaluate(predictions, {evaluator.metricName: 'areaUnderPR'}))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "correct-suicide",
   "metadata": {},
   "source": [
    "### Task 3 - Combine Spark ML and Streaming Analysis\n",
    "Now we have our sentiment prediction model with acceptable predictive performance. The last task is to combine this machine learning mode with spark streaming. That is, with a data stream, we will use the trained model to make real-time predictions. We will still use IMDB reviews and here we will create a simulated data stream from files in a folder, then receive the review stream and predict sentiment using spark structured streaming."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "anticipated-jefferson",
   "metadata": {},
   "source": [
    "We will first create multiple files so that we can simulate a data stream to read files incrementally from a folder. To do that, we use the code below - it creates a folder 'review_stream', turn the spark dataframe into pandas dataframe (this is not a recommended approach but here we use it for convenience and conciseness), split the data into 100 smaller ones, and then save each smaller csv file into the folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "graduate-framing",
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_test_stream = reviews_test.withColumn('review_id', fn.monotonically_increasing_id())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beautiful-nepal",
   "metadata": {},
   "outputs": [],
   "source": [
    "#if you use cluster, do not run the code below, just stream from s3 directory with code for cluster in the next cell \n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "#code for local mode\n",
    "#here we just create a local directory to simulate the data stream\n",
    "if not os.path.exists('review_stream'):\n",
    "    os.mkdir('review_stream')\n",
    "\n",
    "#here we use pandas at local docker to speed up, with spark it is slow\n",
    "df = reviews_test_stream.orderBy('review_id').toPandas()\n",
    "#here we write a small number of rows into each csv file and read data stream from these files\n",
    "#we can split the dataframe into 100 smaller ones so each one has 100 rows\n",
    "for i,chunk in enumerate(np.array_split(df, 100)):\n",
    "    chunk.to_csv('./review_stream/reviews_{}.csv'.format(i), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "radical-pattern",
   "metadata": {},
   "source": [
    "Now we can read from the stream. For convenience, we just take the existing spark dataframe to reuse the schema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "killing-project",
   "metadata": {},
   "outputs": [],
   "source": [
    "#read from data stream in a folder\n",
    "streaming_review = spark.readStream.schema(reviews_test_stream.schema).option(\"maxFilesPerTrigger\", 1).csv('./review_stream')\n",
    "#for cluster\n",
    "#streaming_review = spark.readStream.schema(reviews_test_stream.schema).option(\"maxFilesPerTrigger\", 1).csv('s3://msbx5420-spr21/zhiyiwang/review_stream')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "interpreted-craft",
   "metadata": {},
   "source": [
    "Here from the data stream, we want to know two results. First, how many positive or negative reviews we have received in real time? Second, how many positive or negative reviews in each time window (so we know whether there is a peak of positive or negative review in certain time)? So we will do some calculations, and to capture time window, we will use the current timestamp to create 'processing_time' (the time we receive the data) and apply window on this timestamp."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "great-superintendent",
   "metadata": {},
   "outputs": [],
   "source": [
    "streaming_review_time = streaming_review.withColumn('processing_time', fn.current_timestamp())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nearby-million",
   "metadata": {},
   "outputs": [],
   "source": [
    "#we can just take the pipeline model we have trained to make prediction\n",
    "#[Your Code] to use the last pipeline model with SVM to make predictions on 'streaming_review_time'; save the result as 'streaming_sentiment'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "diagnostic-monroe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a predicted text label from 'predicted column' is the prediction\n",
    "streaming_sentiment = streaming_sentiment.withColumn('predicted', fn.when(fn.col('prediction')==1.0, 'positive').otherwise('negative'))\n",
    "\n",
    "#here we do transformations to get the results we need\n",
    "#first, get total number of positive and negative reviews we have received\n",
    "streaming_sentiment_count = streaming_sentiment.groupBy('predicted').count()\n",
    "\n",
    "#second, still number of positive and negative reviews we received, but by time window (60 seconds)\n",
    "streaming_sentiment_window_count = streaming_sentiment.groupBy(fn.window('processing_time', '60 seconds'), 'predicted').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "frozen-assignment",
   "metadata": {},
   "outputs": [],
   "source": [
    "#now we have two streaming dataframe results\n",
    "print(streaming_sentiment_count.isStreaming)\n",
    "print(streaming_sentiment_window_count.isStreaming)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "subjective-evanescence",
   "metadata": {},
   "outputs": [],
   "source": [
    "#now we can define query to start streaming analysis and set the result table as 'sentiment'\n",
    "query_sentiment = (streaming_sentiment_count.writeStream.format(\"memory\").queryName(\"sentiment\").outputMode(\"complete\").start())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "excellent-directive",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define another query for the result table of windowed positive and negative review counts\n",
    "#[Your Code] to define the second query, name of result table is 'sentiment_window'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "proved-webcam",
   "metadata": {},
   "outputs": [],
   "source": [
    "#query the first result table to monitor real time results\n",
    "spark.sql('select * from sentiment').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accredited-ottawa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#query the second result table to monitor real time results, order the results by window\n",
    "spark.sql('select * from sentiment_window order by window').show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "voluntary-wyoming",
   "metadata": {},
   "outputs": [],
   "source": [
    "#stop query to finish streaming analysis\n",
    "query_sentiment.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cloudy-capability",
   "metadata": {},
   "outputs": [],
   "source": [
    "#stop query to finish streaming analysis\n",
    "query_sentiment_window.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
